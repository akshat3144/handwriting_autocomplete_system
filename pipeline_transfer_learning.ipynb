{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1840441,"sourceType":"datasetVersion","datasetId":1094263},{"sourceId":13382025,"sourceType":"datasetVersion","datasetId":8490658}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =====================================================\n# STAGE 1: OCR with TrOCR (extract text from handwriting PNGs)\n# =====================================================\n\n!pip install -q transformers datasets\n!pip install wandb\n\nimport os\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport torch\nfrom tqdm import tqdm\n\n# Path to dataset (root directory with 657 subdirectories)\ndata_dir = \"/kaggle/input/iam-handwritten-forms-dataset/data\"\noutput_corpus = \"iam_corpus.txt\"\n\n# Load TrOCR pretrained model\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\").to(\"cuda\")\n\ntexts = []\n\n# Iterate through all PNG files with batching\nbatch_size = 16\ntotal_pngs = 0\nfor root, dirs, files in os.walk(data_dir):\n    png_files = [f for f in files if f.endswith(\".png\")]\n    total_pngs += len(png_files)\n    for i in tqdm(range(0, len(png_files), batch_size), desc=f\"OCR in {os.path.basename(root)}\"):\n        batch_files = png_files[i:i + batch_size]\n        images = []\n        for file in batch_files:\n            img_path = os.path.join(root, file)\n            try:\n                image = Image.open(img_path).convert(\"RGB\")\n                images.append(image)\n            except Exception as e:\n                print(f\"Error loading {img_path}: {e}\")\n                continue\n        if not images:\n            continue\n        \n        pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n        generated_ids = model.generate(pixel_values, max_length=512)\n        batch_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n        texts.extend(batch_texts)\n\n# Save extracted text corpus\nwith open(output_corpus, \"w\", encoding=\"utf-8\") as f:\n    for line in texts:\n        f.write(line.strip() + \"\\n\")\n\nprint(f\"OCR complete. Processed {total_pngs} PNGs, extracted {len(texts)} lines of text\")\nprint(\"Example OCR result:\", texts[0] if texts else \"No text extracted\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:14:41.760807Z","iopub.execute_input":"2025-10-14T16:14:41.761629Z","iopub.status.idle":"2025-10-14T16:20:32.605342Z","shell.execute_reply.started":"2025-10-14T16:14:41.761591Z","shell.execute_reply":"2025-10-14T16:20:32.604620Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nOCR in data: 0it [00:00, ?it/s]\nOCR in 515: 100%|██████████| 1/1 [00:00<00:00,  3.14it/s]\nOCR in 248: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\nOCR in 625: 100%|██████████| 1/1 [00:00<00:00,  5.34it/s]\nOCR in 135: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\nOCR in 479: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\nOCR in 183: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\nOCR in 642: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\nOCR in 313: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\nOCR in 600: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s]\nOCR in 086: 100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\nOCR in 466: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\nOCR in 494: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\nOCR in 199: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\nOCR in 480: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\nOCR in 121: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\nOCR in 061: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\nOCR in 262: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\nOCR in 653: 100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\nOCR in 523: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\nOCR in 496: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\nOCR in 192: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\nOCR in 395: 100%|██████████| 1/1 [00:00<00:00,  4.28it/s]\nOCR in 429: 100%|██████████| 1/1 [00:00<00:00,  4.62it/s]\nOCR in 485: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\nOCR in 621: 100%|██████████| 1/1 [00:00<00:00,  5.50it/s]\nOCR in 463: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\nOCR in 330: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\nOCR in 363: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\nOCR in 229: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\nOCR in 664: 100%|██████████| 1/1 [00:00<00:00,  5.18it/s]\nOCR in 048: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\nOCR in 519: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\nOCR in 240: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\nOCR in 244: 100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\nOCR in 419: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\nOCR in 253: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\nOCR in 514: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\nOCR in 651: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\nOCR in 053: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]\nOCR in 406: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\nOCR in 309: 100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\nOCR in 164: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\nOCR in 616: 100%|██████████| 1/1 [00:00<00:00,  5.48it/s]\nOCR in 147: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\nOCR in 145: 100%|██████████| 1/1 [00:00<00:00,  4.62it/s]\nOCR in 213: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\nOCR in 051: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\nOCR in 137: 100%|██████████| 1/1 [00:00<00:00,  4.41it/s]\nOCR in 542: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\nOCR in 095: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 667: 100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\nOCR in 532: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\nOCR in 018: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\nOCR in 611: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\nOCR in 462: 100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\nOCR in 250: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\nOCR in 222: 100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\nOCR in 044: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\nOCR in 336: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\nOCR in 016: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\nOCR in 303: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\nOCR in 447: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\nOCR in 484: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\nOCR in 403: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\nOCR in 007: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\nOCR in 446: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\nOCR in 205: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\nOCR in 207: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\nOCR in 491: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\nOCR in 009: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\nOCR in 012: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 254: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\nOCR in 449: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\nOCR in 206: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\nOCR in 029: 100%|██████████| 1/1 [00:00<00:00,  4.81it/s]\nOCR in 352: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\nOCR in 547: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\nOCR in 187: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\nOCR in 025: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\nOCR in 078: 100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\nOCR in 594: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\nOCR in 476: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\nOCR in 001: 100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\nOCR in 372: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\nOCR in 056: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]\nOCR in 408: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\nOCR in 662: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\nOCR in 367: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\nOCR in 318: 100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\nOCR in 355: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\nOCR in 006: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\nOCR in 120: 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\nOCR in 650: 100%|██████████| 1/1 [00:00<00:00,  4.83it/s]\nOCR in 465: 100%|██████████| 1/1 [00:00<00:00,  4.56it/s]\nOCR in 405: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 435: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\nOCR in 188: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 566: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\nOCR in 109: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\nOCR in 459: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\nOCR in 587: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\nOCR in 359: 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]\nOCR in 247: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\nOCR in 478: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\nOCR in 369: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\nOCR in 272: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\nOCR in 042: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\nOCR in 218: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\nOCR in 000: 100%|██████████| 4/4 [00:14<00:00,  3.61s/it]\nOCR in 127: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\nOCR in 150: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\nOCR in 618: 100%|██████████| 1/1 [00:00<00:00,  5.40it/s]\nOCR in 443: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 503: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s]\nOCR in 379: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\nOCR in 214: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 585: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\nOCR in 533: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\nOCR in 415: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\nOCR in 186: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\nOCR in 444: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\nOCR in 233: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\nOCR in 158: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\nOCR in 392: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\nOCR in 304: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\nOCR in 273: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\nOCR in 458: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\nOCR in 339: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\nOCR in 082: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]\nOCR in 521: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\nOCR in 055: 100%|██████████| 1/1 [00:00<00:00,  4.81it/s]\nOCR in 076: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\nOCR in 146: 100%|██████████| 1/1 [00:00<00:00,  4.30it/s]\nOCR in 209: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\nOCR in 346: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\nOCR in 175: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\nOCR in 416: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\nOCR in 526: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s]\nOCR in 223: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\nOCR in 506: 100%|██████████| 1/1 [00:00<00:00,  4.40it/s]\nOCR in 091: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\nOCR in 453: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\nOCR in 343: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\nOCR in 094: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\nOCR in 259: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\nOCR in 604: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\nOCR in 331: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\nOCR in 292: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\nOCR in 527: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\nOCR in 564: 100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\nOCR in 217: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\nOCR in 324: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\nOCR in 027: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\nOCR in 190: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\nOCR in 490: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 608: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\nOCR in 338: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\nOCR in 230: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\nOCR in 351: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\nOCR in 080: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\nOCR in 508: 100%|██████████| 1/1 [00:00<00:00,  4.62it/s]\nOCR in 617: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s]\nOCR in 350: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\nOCR in 148: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\nOCR in 274: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\nOCR in 404: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\nOCR in 522: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\nOCR in 041: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\nOCR in 036: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\nOCR in 268: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]\nOCR in 257: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\nOCR in 329: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\nOCR in 234: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\nOCR in 541: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]\nOCR in 266: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\nOCR in 634: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\nOCR in 434: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\nOCR in 070: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\nOCR in 399: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\nOCR in 325: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\nOCR in 166: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\nOCR in 375: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\nOCR in 630: 100%|██████████| 1/1 [00:00<00:00,  5.47it/s]\nOCR in 614: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\nOCR in 258: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\nOCR in 524: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\nOCR in 603: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\nOCR in 105: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\nOCR in 302: 100%|██████████| 1/1 [00:00<00:00,  4.28it/s]\nOCR in 544: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\nOCR in 474: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\nOCR in 071: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\nOCR in 516: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\nOCR in 327: 100%|██████████| 1/1 [00:00<00:00,  4.56it/s]\nOCR in 225: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\nOCR in 112: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\nOCR in 286: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\nOCR in 460: 100%|██████████| 1/1 [00:00<00:00,  4.70it/s]\nOCR in 622: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\nOCR in 575: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\nOCR in 567: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\nOCR in 393: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\nOCR in 442: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\nOCR in 417: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\nOCR in 130: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\nOCR in 537: 100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\nOCR in 320: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\nOCR in 169: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\nOCR in 035: 100%|██████████| 1/1 [00:00<00:00,  4.40it/s]\nOCR in 161: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\nOCR in 535: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\nOCR in 151: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\nOCR in 452: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 610: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\nOCR in 026: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\nOCR in 215: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 065: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\nOCR in 202: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\nOCR in 502: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\nOCR in 481: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\nOCR in 278: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\nOCR in 626: 100%|██████████| 1/1 [00:00<00:00,  5.37it/s]\nOCR in 227: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\nOCR in 570: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\nOCR in 200: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\nOCR in 344: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\nOCR in 062: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\nOCR in 412: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s]\nOCR in 084: 100%|██████████| 1/1 [00:00<00:00,  2.76it/s]\nOCR in 189: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]\nOCR in 312: 100%|██████████| 1/1 [00:00<00:00,  2.79it/s]\nOCR in 203: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\nOCR in 114: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\nOCR in 430: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\nOCR in 640: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]\nOCR in 652: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\nOCR in 483: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\nOCR in 308: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s]\nOCR in 212: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\nOCR in 410: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\nOCR in 493: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\nOCR in 432: 100%|██████████| 1/1 [00:00<00:00,  4.70it/s]\nOCR in 178: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 609: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\nOCR in 606: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\nOCR in 569: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\nOCR in 562: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]\nOCR in 193: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\nOCR in 034: 100%|██████████| 1/1 [00:00<00:00,  4.70it/s]\nOCR in 348: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\nOCR in 520: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\nOCR in 517: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\nOCR in 545: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\nOCR in 645: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\nOCR in 378: 100%|██████████| 1/1 [00:00<00:00,  4.47it/s]\nOCR in 333: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\nOCR in 058: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\nOCR in 245: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\nOCR in 631: 100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\nOCR in 060: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\nOCR in 185: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\nOCR in 341: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\nOCR in 068: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\nOCR in 433: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\nOCR in 441: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\nOCR in 195: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 075: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\nOCR in 448: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\nOCR in 583: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\nOCR in 156: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\nOCR in 261: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\nOCR in 512: 100%|██████████| 1/1 [00:00<00:00,  4.56it/s]\nOCR in 425: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\nOCR in 596: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\nOCR in 641: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\nOCR in 620: 100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\nOCR in 518: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\nOCR in 364: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\nOCR in 635: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\nOCR in 160: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\nOCR in 033: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\nOCR in 504: 100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\nOCR in 106: 100%|██████████| 1/1 [00:00<00:00,  4.40it/s]\nOCR in 543: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\nOCR in 624: 100%|██████████| 1/1 [00:00<00:00,  5.50it/s]\nOCR in 249: 100%|██████████| 1/1 [00:00<00:00,  5.34it/s]\nOCR in 647: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\nOCR in 242: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\nOCR in 495: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\nOCR in 558: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\nOCR in 049: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\nOCR in 194: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\nOCR in 138: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\nOCR in 023: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\nOCR in 347: 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\nOCR in 628: 100%|██████████| 1/1 [00:00<00:00,  5.53it/s]\nOCR in 305: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\nOCR in 020: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\nOCR in 291: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\nOCR in 666: 100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\nOCR in 256: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\nOCR in 337: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\nOCR in 243: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\nOCR in 638: 100%|██████████| 1/1 [00:00<00:00,  5.37it/s]\nOCR in 559: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\nOCR in 360: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\nOCR in 316: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\nOCR in 473: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\nOCR in 591: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]\nOCR in 184: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\nOCR in 290: 100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\nOCR in 226: 100%|██████████| 1/1 [00:00<00:00,  4.30it/s]\nOCR in 477: 100%|██████████| 1/1 [00:00<00:00,  4.81it/s]\nOCR in 598: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 232: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\nOCR in 576: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\nOCR in 197: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 211: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\nOCR in 235: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 013: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\nOCR in 099: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\nOCR in 671: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\nOCR in 445: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\nOCR in 050: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\nOCR in 391: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\nOCR in 096: 100%|██████████| 1/1 [00:00<00:00,  4.37it/s]\nOCR in 557: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\nOCR in 238: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\nOCR in 097: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\nOCR in 113: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\nOCR in 052: 100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\nOCR in 362: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\nOCR in 282: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\nOCR in 411: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s]\nOCR in 563: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\nOCR in 361: 100%|██████████| 1/1 [00:00<00:00,  4.70it/s]\nOCR in 221: 100%|██████████| 1/1 [00:00<00:00,  4.42it/s]\nOCR in 328: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\nOCR in 498: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\nOCR in 580: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\nOCR in 307: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\nOCR in 354: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\nOCR in 548: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\nOCR in 353: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\nOCR in 615: 100%|██████████| 1/1 [00:00<00:00,  5.37it/s]\nOCR in 342: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\nOCR in 389: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\nOCR in 239: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\nOCR in 073: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]\nOCR in 008: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\nOCR in 601: 100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\nOCR in 066: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\nOCR in 108: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\nOCR in 179: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 216: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\nOCR in 439: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 593: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\nOCR in 139: 100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\nOCR in 162: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 104: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\nOCR in 157: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\nOCR in 586: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\nOCR in 002: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\nOCR in 659: 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\nOCR in 129: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\nOCR in 418: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\nOCR in 551: 100%|██████████| 1/1 [00:09<00:00,  9.95s/it]\nOCR in 335: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\nOCR in 655: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\nOCR in 277: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 201: 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]\nOCR in 264: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\nOCR in 470: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\nOCR in 168: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\nOCR in 251: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\nOCR in 124: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\nOCR in 451: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\nOCR in 670: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\nOCR in 612: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\nOCR in 255: 100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\nOCR in 283: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\nOCR in 654: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\nOCR in 123: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\nOCR in 597: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]\nOCR in 345: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\nOCR in 468: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\nOCR in 237: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\nOCR in 176: 100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\nOCR in 436: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\nOCR in 067: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\nOCR in 590: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\nOCR in 210: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\nOCR in 371: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\nOCR in 022: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\nOCR in 043: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\nOCR in 133: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\nOCR in 356: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\nOCR in 285: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\nOCR in 629: 100%|██████████| 1/1 [00:00<00:00,  5.47it/s]\nOCR in 054: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\nOCR in 111: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\nOCR in 115: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s]\nOCR in 455: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\nOCR in 577: 100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\nOCR in 314: 100%|██████████| 1/1 [00:00<00:00,  4.07it/s]\nOCR in 501: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\nOCR in 279: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\nOCR in 467: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\nOCR in 231: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\nOCR in 081: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\nOCR in 047: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\nOCR in 450: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\nOCR in 198: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\nOCR in 270: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\nOCR in 131: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\nOCR in 117: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\nOCR in 280: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\nOCR in 349: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\nOCR in 011: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\nOCR in 385: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\nOCR in 555: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\nOCR in 668: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\nOCR in 219: 100%|██████████| 1/1 [00:00<00:00,  4.35it/s]\nOCR in 171: 100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\nOCR in 180: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\nOCR in 004: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\nOCR in 377: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\nOCR in 424: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\nOCR in 102: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\nOCR in 087: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\nOCR in 140: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\nOCR in 174: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\nOCR in 390: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\nOCR in 072: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\nOCR in 627: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\nOCR in 397: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\nOCR in 561: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\nOCR in 289: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\nOCR in 550: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\nOCR in 556: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\nOCR in 167: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s]\nOCR in 540: 100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\nOCR in 487: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 509: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\nOCR in 665: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\nOCR in 376: 100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\nOCR in 382: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\nOCR in 536: 100%|██████████| 1/1 [00:00<00:00,  5.14it/s]\nOCR in 428: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\nOCR in 241: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\nOCR in 149: 100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\nOCR in 107: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\nOCR in 370: 100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\nOCR in 155: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\nOCR in 637: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\nOCR in 426: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\nOCR in 021: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\nOCR in 015: 100%|██████████| 1/1 [00:00<00:00,  4.30it/s]\nOCR in 059: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\nOCR in 573: 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\nOCR in 141: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\nOCR in 366: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\nOCR in 383: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\nOCR in 299: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\nOCR in 421: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]\nOCR in 546: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\nOCR in 423: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\nOCR in 387: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\nOCR in 401: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\nOCR in 236: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\nOCR in 014: 100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\nOCR in 529: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\nOCR in 505: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\nOCR in 208: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\nOCR in 528: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\nOCR in 340: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\nOCR in 661: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\nOCR in 560: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\nOCR in 119: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\nOCR in 482: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\nOCR in 398: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\nOCR in 619: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\nOCR in 394: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\nOCR in 643: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\nOCR in 159: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\nOCR in 039: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\nOCR in 663: 100%|██████████| 1/1 [00:00<00:00,  5.14it/s]\nOCR in 040: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\nOCR in 300: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\nOCR in 539: 100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\nOCR in 489: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 182: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\nOCR in 579: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\nOCR in 090: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\nOCR in 265: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\nOCR in 143: 100%|██████████| 1/1 [00:00<00:00,  4.35it/s]\nOCR in 064: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\nOCR in 267: 100%|██████████| 1/1 [00:00<00:00,  5.07it/s]\nOCR in 063: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\nOCR in 472: 100%|██████████| 1/1 [00:00<00:00,  4.42it/s]\nOCR in 589: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\nOCR in 414: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\nOCR in 599: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 386: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\nOCR in 031: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]\nOCR in 595: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\nOCR in 464: 100%|██████████| 1/1 [00:00<00:00,  4.53it/s]\nOCR in 644: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\nOCR in 296: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\nOCR in 400: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\nOCR in 310: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\nOCR in 092: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\nOCR in 295: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\nOCR in 572: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]\nOCR in 531: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\nOCR in 269: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\nOCR in 457: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\nOCR in 584: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\nOCR in 293: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\nOCR in 017: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\nOCR in 454: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\nOCR in 152: 100%|██████████| 1/1 [00:09<00:00,  9.31s/it]\nOCR in 373: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\nOCR in 103: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\nOCR in 322: 100%|██████████| 1/1 [00:00<00:00,  4.53it/s]\nOCR in 581: 100%|██████████| 1/1 [00:00<00:00,  5.03it/s]\nOCR in 456: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\nOCR in 173: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\nOCR in 125: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\nOCR in 228: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\nOCR in 607: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\nOCR in 003: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\nOCR in 287: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\nOCR in 144: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]\nOCR in 613: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\nOCR in 170: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\nOCR in 083: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\nOCR in 019: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\nOCR in 422: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\nOCR in 431: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\nOCR in 163: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\nOCR in 024: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\nOCR in 252: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\nOCR in 500: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\nOCR in 069: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\nOCR in 409: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\nOCR in 602: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\nOCR in 224: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\nOCR in 552: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\nOCR in 639: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\nOCR in 365: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\nOCR in 669: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]\nOCR in 413: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\nOCR in 093: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\nOCR in 407: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\nOCR in 571: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\nOCR in 649: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\nOCR in 317: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\nOCR in 380: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\nOCR in 554: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\nOCR in 276: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\nOCR in 497: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\nOCR in 142: 100%|██████████| 1/1 [00:00<00:00,  4.47it/s]\nOCR in 321: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\nOCR in 126: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\nOCR in 154: 100%|██████████| 1/1 [00:03<00:00,  3.51s/it]\nOCR in 089: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\nOCR in 461: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\nOCR in 037: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\nOCR in 005: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\nOCR in 134: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\nOCR in 402: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\nOCR in 128: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\nOCR in 538: 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\nOCR in 605: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\nOCR in 632: 100%|██████████| 1/1 [00:00<00:00,  5.37it/s]\nOCR in 588: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\nOCR in 288: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\nOCR in 530: 100%|██████████| 1/1 [00:00<00:00,  4.63it/s]\nOCR in 098: 100%|██████████| 1/1 [00:00<00:00,  4.37it/s]\nOCR in 510: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\nOCR in 513: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\nOCR in 511: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\nOCR in 633: 100%|██████████| 1/1 [00:00<00:00,  5.37it/s]\nOCR in 384: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\nOCR in 499: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\nOCR in 204: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\nOCR in 592: 100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\nOCR in 122: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\nOCR in 396: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\nOCR in 420: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\nOCR in 565: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\nOCR in 568: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\nOCR in 469: 100%|██████████| 1/1 [00:00<00:00,  4.37it/s]\nOCR in 486: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\nOCR in 220: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\nOCR in 582: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\nOCR in 525: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\nOCR in 100: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\nOCR in 578: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\nOCR in 294: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\nOCR in 427: 100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\nOCR in 110: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\nOCR in 648: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\nOCR in 332: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\nOCR in 177: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\nOCR in 281: 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\nOCR in 046: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\nOCR in 534: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\nOCR in 045: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]\nOCR in 440: 100%|██████████| 1/1 [00:00<00:00,  4.37it/s]\nOCR in 368: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\nOCR in 492: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\nOCR in 658: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\nOCR in 196: 100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\nOCR in 549: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\nOCR in 388: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\nOCR in 010: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\nOCR in 475: 100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\nOCR in 153: 100%|██████████| 1/1 [00:02<00:00,  2.19s/it]\nOCR in 334: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\nOCR in 132: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\nOCR in 088: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\nOCR in 181: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\nOCR in 315: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\nOCR in 574: 100%|██████████| 1/1 [00:00<00:00,  5.02it/s]\nOCR in 116: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\nOCR in 301: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\nOCR in 263: 100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\nOCR in 553: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\nOCR in 077: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\nOCR in 028: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\nOCR in 471: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\nOCR in 488: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\nOCR in 323: 100%|██████████| 1/1 [00:00<00:00,  4.53it/s]\nOCR in 038: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\nOCR in 623: 100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\nOCR in 636: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\nOCR in 275: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\nOCR in 660: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\nOCR in 326: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\nOCR in 357: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\nOCR in 297: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\nOCR in 118: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\nOCR in 074: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\nOCR in 191: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\nOCR in 165: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\nOCR in 079: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\nOCR in 032: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\nOCR in 319: 100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\nOCR in 030: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\nOCR in 172: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\nOCR in 298: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\nOCR in 136: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\nOCR in 260: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\nOCR in 085: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\nOCR in 246: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]","output_type":"stream"},{"name":"stdout","text":"OCR complete. Processed 1539 PNGs, extracted 1539 lines of text\nExample OCR result: 0 0000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# =====================================================\n# STAGE 2a: Train LSTM-based Language Model\n# =====================================================\n\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Load text corpus and preprocess\nwith open(output_corpus, \"r\", encoding=\"utf-8\") as f:\n    corpus = f.read()\n\n# Filter out non-printable chars and normalize\nimport re\ncorpus = re.sub(r'[^\\x20-\\x7E]', ' ', corpus)  # Keep ASCII printable\nchars = sorted(list(set(corpus)))\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for ch, i in stoi.items()}\n\ndef encode(s): return [stoi.get(c, 0) for c in s]  # Default to 0 for unknown\ndef decode(l): return ''.join([itos.get(i, '') for i in l])\n\ndata = torch.tensor(encode(corpus), dtype=torch.long)\nsplit = int(0.9 * len(data))\ntrain_data, val_data = data[:split], data[split:]\n\nblock_size = 128\nbatch_size = 64\n\ndef get_batch(split):\n    d = train_data if split == 'train' else val_data\n    ix = torch.randint(len(d) - block_size, (batch_size,))\n    x = torch.stack([d[i:i+block_size] for i in ix])\n    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n    return x.cuda(), y.cuda()\n\nclass CharLSTM(nn.Module):\n    def __init__(self, vocab_size, hidden_size=256, n_layers=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x, hidden=None):\n        if hidden is None:\n            batch_size = x.size(0)\n            h0 = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).cuda()\n            c0 = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).cuda()\n            hidden = (h0, c0)\n        x = self.embed(x)\n        out, hidden = self.lstm(x, hidden)\n        logits = self.fc(out)\n        return logits, hidden\n\nvocab_size = len(chars)\nmodel_lstm = CharLSTM(vocab_size).cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_lstm.parameters(), lr=0.003)\n\n# Train with validation\nfor epoch in range(3):\n    model_lstm.train()\n    total_train_loss = 0\n    for _ in range(100):  # Limit iterations for demo\n        x, y = get_batch('train')\n        optimizer.zero_grad()\n        logits, _ = model_lstm(x)\n        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_train_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Train Loss: {total_train_loss/100:.4f}\")\n    \n    model_lstm.eval()\n    val_loss = 0\n    with torch.no_grad():\n        x, y = get_batch('val')\n        logits, _ = model_lstm(x)\n        val_loss = criterion(logits.view(-1, vocab_size), y.view(-1)).item()\n    print(f\"Val Loss: {val_loss:.4f}\")\n\ntorch.save(model_lstm.state_dict(), \"lstm_lm.pth\")\nprint(\"LSTM LM trained and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:20:32.606945Z","iopub.execute_input":"2025-10-14T16:20:32.607253Z","iopub.status.idle":"2025-10-14T16:20:41.149074Z","shell.execute_reply.started":"2025-10-14T16:20:32.607230Z","shell.execute_reply":"2025-10-14T16:20:41.148194Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Train Loss: 0.7941\nVal Loss: 0.7910\nEpoch 2, Train Loss: 0.2664\nVal Loss: 0.6952\nEpoch 3, Train Loss: 0.2157\nVal Loss: 0.8229\nLSTM LM trained and saved.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# =====================================================\n# STAGE 2b: Fine-tune DistilGPT2 on OCR corpus\n# =====================================================\n\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nimport os\n\n# Prepare dataset\ndataset = Dataset.from_dict({\"text\": texts})\ntokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\ntokenizer.pad_token = tokenizer.eos_token  # Handle padding\n\ndef tokenize(batch):\n    # Tokenize the text\n    encodings = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n    # Create labels by shifting input_ids (predict next token)\n    encodings[\"labels\"] = encodings[\"input_ids\"].clone()\n    return encodings.data  # Return as dict without extra tensor structure\n\ntokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\nprint(f\"Tokenized dataset size: {len(tokenized_dataset)}\")\n\n# Split train/val\ntrain_test = tokenized_dataset.train_test_split(test_size=0.1)\ntrain_dataset = train_test[\"train\"]\neval_dataset = train_test[\"test\"]\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Eval dataset size: {len(eval_dataset)}\")\n\n# Load model and move to GPU\nmodel_gpt = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(\"cuda\")\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2-finetuned\",\n    do_eval=True,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=50,\n    save_strategy=\"epoch\"\n)\n\n# Disable WandB to avoid conflicts\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Initialize Trainer with GPU model\ntrainer = Trainer(\n    model=model_gpt,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset\n)\n\nprint(\"Starting training...\")\ntrainer.train()\nprint(\"Training completed.\")\nmodel_gpt.save_pretrained(\"gpt2-finetuned\")\ntokenizer.save_pretrained(\"gpt2-finetuned\")\n\nprint(\"DistilGPT-2 fine-tuned and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:20:41.150122Z","iopub.execute_input":"2025-10-14T16:20:41.150437Z","iopub.status.idle":"2025-10-14T16:21:26.702031Z","shell.execute_reply.started":"2025-10-14T16:20:41.150407Z","shell.execute_reply":"2025-10-14T16:21:26.701224Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1539 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51272ed3ffe14add8e0c2e405ea62342"}},"metadata":{}},{"name":"stdout","text":"Tokenized dataset size: 1539\nTrain dataset size: 1385\nEval dataset size: 154\n","output_type":"stream"},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='174' max='174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [174/174 00:42, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.231900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.010700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training completed.\nDistilGPT-2 fine-tuned and saved.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# =====================================================\n# STAGE 2c: Evaluate Language Models\n# =====================================================\n\nfrom math import exp\nimport numpy as np\n\n# LSTM Perplexity and Top-k Accuracy\ndef compute_metrics_lstm():\n    model_lstm.eval()\n    x, y = get_batch('val')\n    with torch.no_grad():\n        logits, _ = model_lstm(x)\n        loss = criterion(logits.view(-1, vocab_size), y.view(-1)).item()\n        ppl = exp(loss)\n        _, topk_ids = logits.topk(5, dim=-1)  # Top-5\n        y_flat = y.view(-1)\n        topk_flat = topk_ids.view(-1, 5)\n        top1_correct = (topk_ids[:, :, 0] == y).float().mean().item() * 100\n        top5_correct = (topk_flat == y_flat.unsqueeze(-1)).any(dim=-1).float().mean().item() * 100\n    return {\n        'Perplexity (↓)': ppl,\n        'Top-1 Accuracy (↑)': top1_correct,\n        'Top-5 Accuracy (↑)': top5_correct\n    }\n\nlstm_metrics = compute_metrics_lstm()\nprint(\"LSTM Metrics:\", {k: f\"{v:.2f}\" for k, v in lstm_metrics.items()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:21:26.703635Z","iopub.execute_input":"2025-10-14T16:21:26.703934Z","iopub.status.idle":"2025-10-14T16:21:26.729650Z","shell.execute_reply.started":"2025-10-14T16:21:26.703914Z","shell.execute_reply":"2025-10-14T16:21:26.728966Z"}},"outputs":[{"name":"stdout","text":"LSTM Metrics: {'Perplexity (↓)': '2.53', 'Top-1 Accuracy (↑)': '78.61', 'Top-5 Accuracy (↑)': '95.96'}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef compute_metrics_gpt2(eval_dataset, tokenizer, model_gpt, batch_size=8, device=\"cuda\"):\n    model_gpt.eval()\n    losses = []\n    top1_correct = 0\n    top5_correct = 0\n    total = 0\n\n    # Determine dataset type\n    is_hf_dataset = hasattr(eval_dataset, \"column_names\")  # Hugging Face dataset\n\n    for i in tqdm(range(0, len(eval_dataset), batch_size), desc=\"GPT-2 Eval\"):\n        # Get batch depending on type\n        if is_hf_dataset:\n            # Hugging Face dataset -> dict of columns\n            batch_input_ids = eval_dataset[i:i + batch_size][\"input_ids\"]\n            input_ids_list = [torch.tensor(x, dtype=torch.long) for x in batch_input_ids]\n            input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n        else:\n            batch = eval_dataset[i:i + batch_size]\n            if isinstance(batch[0], dict) and \"input_ids\" in batch[0]:\n                input_ids_list = [torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch]\n                input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n            elif isinstance(batch[0], list) or isinstance(batch[0], torch.Tensor):\n                input_ids_list = [torch.tensor(x, dtype=torch.long) for x in batch]\n                input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n            elif isinstance(batch[0], str):\n                encodings = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n                input_ids = encodings[\"input_ids\"].to(device)\n            else:\n                raise ValueError(f\"Unsupported data type: {type(batch[0])}\")\n\n        # Shift for next-token prediction\n        labels = input_ids[:, 1:].contiguous()\n        input_ids = input_ids[:, :-1].contiguous()\n\n        with torch.no_grad():\n            outputs = model_gpt(input_ids, labels=labels)\n            loss = outputs.loss\n            losses.append(loss.item())\n\n            logits = outputs.logits\n            _, topk_ids = logits.topk(5, dim=-1)\n            top1_correct += (topk_ids[:, :, 0] == labels).float().sum().item()\n            top5_correct += (topk_ids == labels.unsqueeze(-1)).any(dim=-1).float().sum().item()\n            total += labels.numel()\n\n    ppl = exp(np.mean(losses))\n    top1_acc = (top1_correct / total) * 100\n    top5_acc = (top5_correct / total) * 100\n\n    return {\n        'Perplexity (↓)': ppl,\n        'Top-1 Accuracy (↑)': top1_acc,\n        'Top-5 Accuracy (↑)': top5_acc\n    }\n\n# Example usage\ngpt2_metrics = compute_metrics_gpt2(eval_dataset, tokenizer, model_gpt, batch_size=8, device=\"cuda\")\nprint(\"GPT-2 Metrics:\", {k: f\"{v:.2f}\" for k, v in gpt2_metrics.items()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:21:26.730417Z","iopub.execute_input":"2025-10-14T16:21:26.730755Z","iopub.status.idle":"2025-10-14T16:21:28.016847Z","shell.execute_reply.started":"2025-10-14T16:21:26.730726Z","shell.execute_reply":"2025-10-14T16:21:28.016115Z"}},"outputs":[{"name":"stderr","text":"GPT-2 Eval: 100%|██████████| 20/20 [00:01<00:00, 15.72it/s]","output_type":"stream"},{"name":"stdout","text":"GPT-2 Metrics: {'Perplexity (↓)': '2.82', 'Top-1 Accuracy (↑)': '99.01', 'Top-5 Accuracy (↑)': '99.87'}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# =====================================================\n# STAGE 3: Predict Next Words with Confidence\n# =====================================================\n\nimport torch\nfrom transformers import AutoTokenizer\nimport numpy as np\n\ndef predict_next_words(input_text, tokenizer, model_lstm, model_gpt, k=5, max_length=10):\n    # Ensure models are in eval mode\n    model_lstm.eval()\n    model_gpt.eval()\n\n    # Tokenize input for both models\n    # For LSTM (character-based)\n    chars = sorted(list(set(\"\".join(texts) + input_text)))\n    stoi = {ch: i for i, ch in enumerate(chars)}\n    itos = {i: ch for ch, i in stoi.items()}\n    input_ids_lstm = torch.tensor([stoi.get(c, 0) for c in input_text], dtype=torch.long).unsqueeze(0).to(\"cuda\")\n    \n    # For GPT-2 (token-based)\n    inputs_gpt = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n    input_ids_gpt = inputs_gpt[\"input_ids\"].to(\"cuda\")\n    attention_mask = inputs_gpt[\"attention_mask\"].to(\"cuda\")\n\n    # Predict with LSTM\n    with torch.no_grad():\n        logits, _ = model_lstm(input_ids_lstm)\n        probs = torch.softmax(logits[:, -1, :], dim=-1)  # Probabilities for the last character\n        topk_values_lstm, topk_indices_lstm = probs.topk(k)\n        next_chars_lstm = [itos[i] for i in topk_indices_lstm[0].cpu().numpy()]\n        confidences_lstm = topk_values_lstm[0].cpu().numpy()\n\n    # Predict with GPT-2\n    with torch.no_grad():\n        outputs = model_gpt(input_ids_gpt, attention_mask=attention_mask)\n        logits = outputs.logits[:, -1, :]  # Logits for the last token\n        probs = torch.softmax(logits, dim=-1)\n        topk_values_gpt, topk_indices_gpt = probs.topk(k)\n        next_tokens_gpt = tokenizer.convert_ids_to_tokens(topk_indices_gpt[0].cpu().numpy())\n        confidences_gpt = topk_values_gpt[0].cpu().numpy()\n\n    # Filter and map to words (simplified word boundary assumption)\n    def get_words_from_chars(chars, confs):\n        words = []\n        current_word = \"\"\n        current_conf = 1.0\n        for char, conf in zip(chars, confs):\n            if char == \" \" or len(current_word) > max_length:\n                if current_word:\n                    words.append((current_word, current_conf))\n                current_word = \"\"\n                current_conf = 1.0\n            else:\n                current_word += char\n                current_conf *= conf\n        if current_word:\n            words.append((current_word, current_conf))\n        return words[:k]\n\n    def get_words_from_tokens(tokens, confs):\n        words = []\n        for token, conf in zip(tokens, confs):\n            if token.startswith(\"##\") or token in [\".\", \",\", \"!\", \"?\"]:\n                continue\n            word = token.replace(\"##\", \"\")\n            words.append((word, conf))\n        return words[:k]\n\n    # Process predictions\n    next_words_lstm = get_words_from_chars(next_chars_lstm, confidences_lstm)\n    next_words_gpt = get_words_from_tokens(next_tokens_gpt, confidences_gpt)\n\n    # Print results\n    print(f\"\\nInput: '{input_text}'\")\n    print(\"LSTM Next Words with Confidence:\")\n    for word, conf in next_words_lstm:\n        print(f\"  - {word}: {conf:.4f}\")\n    print(\"GPT-2 Next Words with Confidence:\")\n    for word, conf in next_words_gpt:\n        print(f\"  - {word}: {conf:.4f}\")\n\n# Example usage\ncustom_input = \"Please write a\"\npredict_next_words(custom_input, tokenizer, model_lstm, model_gpt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:21:28.017612Z","iopub.execute_input":"2025-10-14T16:21:28.017899Z","iopub.status.idle":"2025-10-14T16:21:28.039947Z","shell.execute_reply.started":"2025-10-14T16:21:28.017881Z","shell.execute_reply":"2025-10-14T16:21:28.039345Z"}},"outputs":[{"name":"stdout","text":"\nInput: 'Please write a'\nLSTM Next Words with Confidence:\n  - nspl: 0.0000\nGPT-2 Next Words with Confidence:\n  - Ġnote: 0.1753\n  - Ġcomment: 0.1711\n  - Ġletter: 0.1178\n  - Ġresponse: 0.0509\n  - Ġreview: 0.0427\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# =====================================================\n#Predict Next Words from Image Input\n# =====================================================\n\nfrom PIL import Image\nimport requests  # If loading from URL; otherwise use local file path\nfrom io import BytesIO\n\n# Load TrOCR if not already loaded\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\ntrocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\").to(\"cuda\")\n\ndef extract_text_from_image(image_path_or_url):\n    \"\"\"\n    Extracts text from a handwritten image using TrOCR.\n    - image_path_or_url: Local file path or URL to the image.\n    \"\"\"\n    try:\n        if image_path_or_url.startswith(\"http\"):  # Load from URL\n            response = requests.get(image_path_or_url)\n            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n        else:  # Local file\n            image = Image.open(image_path_or_url).convert(\"RGB\")\n        \n        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n        generated_ids = trocr_model.generate(pixel_values, max_length=512)\n        extracted_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        \n        print(f\"Extracted Text: '{extracted_text}'\")\n        return extracted_text.strip()\n    except Exception as e:\n        print(f\"Error processing image: {e}\")\n        return \"\"\n\n# Modified predict_next_words to handle image input\ndef predict_next_words_from_image(image_path_or_url, tokenizer, model_lstm, model_gpt, k=5, max_length=10):\n    input_text = extract_text_from_image(image_path_or_url)\n    if not input_text:\n        print(\"No text extracted from image.\")\n        return\n    \n    # Now use the existing predict_next_words function\n    predict_next_words(input_text, tokenizer, model_lstm, model_gpt, k, max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:21:28.040638Z","iopub.execute_input":"2025-10-14T16:21:28.040904Z","iopub.status.idle":"2025-10-14T16:21:29.824136Z","shell.execute_reply.started":"2025-10-14T16:21:28.040889Z","shell.execute_reply":"2025-10-14T16:21:29.823494Z"}},"outputs":[{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"image_input = \"/kaggle/input/test-image/WhatsApp Image 2025-10-14 at 17.40.54_ed2fab4d.jpg\"\npredict_next_words_from_image(image_input, tokenizer, model_lstm, model_gpt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T16:21:29.824969Z","iopub.execute_input":"2025-10-14T16:21:29.825230Z","iopub.status.idle":"2025-10-14T16:21:30.003309Z","shell.execute_reply.started":"2025-10-14T16:21:29.825203Z","shell.execute_reply":"2025-10-14T16:21:30.002492Z"}},"outputs":[{"name":"stdout","text":"Extracted Text: '2 . Please write a'\n\nInput: '2 . Please write a'\nLSTM Next Words with Confidence:\n  - nspl: 0.0000\nGPT-2 Next Words with Confidence:\n  - Ġnote: 0.1793\n  - Ġcomment: 0.0898\n  - Ġcopy: 0.0749\n  - Ġletter: 0.0371\n  - Ġreview: 0.0259\n","output_type":"stream"}],"execution_count":16}]}