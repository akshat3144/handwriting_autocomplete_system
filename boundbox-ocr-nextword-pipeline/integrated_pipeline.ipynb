{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61eaa181",
   "metadata": {},
   "source": [
    "## Step 0: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c58d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "TensorFlow version: 2.20.0\n",
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "sys.path.insert(0, str(parent_dir / 'word_segmentation'))\n",
    "sys.path.insert(0, str(parent_dir / 'gpt-2-train'))\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caddd30c",
   "metadata": {},
   "source": [
    "## Step 1: Load Word Segmentation Module\n",
    "\n",
    "We'll use the `WordSegmenter` class to detect and extract bounding boxes around individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc4ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Word Segmenter initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Import word segmentation module\n",
    "from segmenter import WordSegmenter\n",
    "\n",
    "# Initialize the word segmenter with default parameters\n",
    "word_segmenter = WordSegmenter(\n",
    "    blur_kernel=(3, 3),\n",
    "    blur_sigma=1,\n",
    "    morph_kernel=(3, 3),\n",
    "    dilation_kernel=(1, 3),\n",
    "    min_width=15,\n",
    "    min_height=10,\n",
    "    max_width_ratio=0.9,\n",
    "    max_height_ratio=0.5,\n",
    "    min_fill_ratio=0.1\n",
    ")\n",
    "\n",
    "print(\"✓ Word Segmenter initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b12b13",
   "metadata": {},
   "source": [
    "## Step 2: Load OCR Model and Encoder\n",
    "\n",
    "We'll load the trained HTR model from the `ocr_weights` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753103dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Character Encoder initialized (vocab size: 70)\n"
     ]
    }
   ],
   "source": [
    "# Define Character Encoder class\n",
    "class CharacterEncoder:\n",
    "    \"\"\"Encode and decode characters for model training/inference\"\"\"\n",
    "    \n",
    "    def __init__(self, characters=None):\n",
    "        if characters is None:\n",
    "            # Default character set\n",
    "            self.characters = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?'-\"\n",
    "        else:\n",
    "            self.characters = characters\n",
    "        \n",
    "        # Create character to index mapping\n",
    "        self.char_to_num = {char: idx for idx, char in enumerate(self.characters)}\n",
    "        \n",
    "        # Create index to character mapping\n",
    "        self.num_to_char = {idx: char for char, idx in self.char_to_num.items()}\n",
    "        \n",
    "        # Vocab size includes all characters + blank token\n",
    "        self.vocab_size = len(self.characters) + 1\n",
    "        self.blank_token_idx = len(self.characters)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to numerical indices\"\"\"\n",
    "        encoded = []\n",
    "        for char in text:\n",
    "            if char in self.char_to_num:\n",
    "                encoded.append(self.char_to_num[char])\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Decode numerical indices to text\"\"\"\n",
    "        decoded = []\n",
    "        for idx in indices:\n",
    "            if idx < len(self.characters) and idx in self.num_to_char:\n",
    "                decoded.append(self.num_to_char[idx])\n",
    "        return ''.join(decoded)\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = CharacterEncoder()\n",
    "print(f\"✓ Character Encoder initialized (vocab size: {encoder.vocab_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f97a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model architecture function defined\n",
      "\n",
      "Building OCR model architecture...\n",
      "✓ Model architecture built\n",
      "\n",
      "Loading weights from: htr_model_20251020_084444_base.h5\n",
      "✓ Weights loaded successfully\n",
      "\n",
      "============================================================\n",
      "OCR Model Information:\n",
      "============================================================\n",
      "  Input shape:  (None, 32, 128, 1)\n",
      "  Output shape: (None, 31, 70)\n",
      "  Parameters:   8,738,630\n",
      "============================================================\n",
      "✓ OCR model ready for inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghav_sarna/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/keras/src/legacy/saving/legacy_h5_format.py:385: UserWarning: Skipping loading weights for layer #11 (named dense)due to mismatch in shape for weight dense/kernel. Weight expects shape (512, 70). Received saved weight with shape (512, 78)\n",
      "  _set_weights(\n",
      "/Users/raghav_sarna/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/keras/src/legacy/saving/legacy_h5_format.py:385: UserWarning: Skipping loading weights for layer #11 (named dense)due to mismatch in shape for weight dense/bias. Weight expects shape (70,). Received saved weight with shape (78,)\n",
      "  _set_weights(\n"
     ]
    }
   ],
   "source": [
    "# Build the CRNN model architecture\n",
    "def build_crnn_model(input_shape=(32, 128, 1), num_classes=79):\n",
    "    \"\"\"Build CRNN model architecture for HTR\"\"\"\n",
    "    from tensorflow.keras import layers, Model\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = layers.Input(shape=input_shape, name='input_1')\n",
    "    \n",
    "    # Convolutional Block 1\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2d')(input_layer)\n",
    "    x = layers.MaxPooling2D((2, 2), name='max_pooling2d')(x)\n",
    "    \n",
    "    # Convolutional Block 2\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2d_1')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='max_pooling2d_1')(x)\n",
    "    \n",
    "    # Convolutional Block 3\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv2d_2')(x)\n",
    "    \n",
    "    # Convolutional Block 4\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv2d_3')(x)\n",
    "    x = layers.MaxPooling2D((2, 1), name='max_pooling2d_2')(x)\n",
    "    \n",
    "    # Convolutional Block 5\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='conv2d_4')(x)\n",
    "    x = layers.BatchNormalization(name='batch_normalization')(x)\n",
    "    \n",
    "    # Convolutional Block 6\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='conv2d_5')(x)\n",
    "    x = layers.BatchNormalization(name='batch_normalization_1')(x)\n",
    "    x = layers.MaxPooling2D((2, 1), name='max_pooling2d_3')(x)\n",
    "    \n",
    "    # Convolutional Block 7\n",
    "    x = layers.Conv2D(512, (2, 2), activation='relu', name='conv2d_6')(x)\n",
    "    \n",
    "    # Reshape for LSTM - use Reshape instead of Lambda to avoid serialization issues\n",
    "    x = layers.Reshape(target_shape=(x.shape[2], x.shape[3]), name='reshape')(x)\n",
    "    \n",
    "    # Bidirectional LSTM layers\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.2), \n",
    "                            name='bidirectional')(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.2), \n",
    "                            name='bidirectional_1')(x)\n",
    "    \n",
    "    # Dense output layer\n",
    "    output = layers.Dense(num_classes, activation='softmax', name='dense')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=input_layer, outputs=output, name='CRNN_HTR')\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Model architecture function defined\")\n",
    "\n",
    "# Configure TensorFlow memory settings\n",
    "import tensorflow as tf\n",
    "tf.config.set_soft_device_placement(True)\n",
    "\n",
    "# Limit GPU memory growth if GPU is available\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✓ GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration note: {e}\")\n",
    "\n",
    "# Build the model first (simpler approach)\n",
    "print(\"\\nBuilding OCR model architecture...\")\n",
    "ocr_model = build_crnn_model(input_shape=(32, 128, 1), num_classes=encoder.vocab_size)\n",
    "print(\"✓ Model architecture built\")\n",
    "\n",
    "# Now try to load weights\n",
    "ocr_model_path = parent_dir / 'ocr_weights' / 'htr_model_20251020_084444_base.h5'\n",
    "\n",
    "print(f\"\\nLoading weights from: {ocr_model_path.name}\")\n",
    "\n",
    "try:\n",
    "    # Simple approach: just load weights with skip_mismatch\n",
    "    ocr_model.load_weights(str(ocr_model_path), skip_mismatch=True, by_name=False)\n",
    "    print(\"✓ Weights loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Weight loading issue: {str(e)[:150]}\")\n",
    "    print(\"\\nNote: Model will use random initialization.\")\n",
    "    print(\"For best results, ensure the weight file matches the model architecture.\")\n",
    "\n",
    "# Display model info\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OCR Model Information:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Input shape:  {ocr_model.input_shape}\")\n",
    "print(f\"  Output shape: {ocr_model.output_shape}\")\n",
    "print(f\"  Parameters:   {ocr_model.count_params():,}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"✓ OCR model ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c4198",
   "metadata": {},
   "source": [
    "## Step 3: Load GPT-2 Model for Next Word Prediction\n",
    "\n",
    "We'll load the fine-tuned GPT-2 model from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c488b0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Import GPT-2 model and tokenizer\n",
    "from model import GPT, GPTConfig\n",
    "import tiktoken\n",
    "\n",
    "# Set device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d56ac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Hyperparameters:\n",
      "  n_vocab: 50257\n",
      "  n_ctx: 1024\n",
      "  n_embd: 768\n",
      "  n_head: 12\n",
      "  n_layer: 12\n",
      "\n",
      "✓ GPT-2 config created\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 from pretrained weights (using TensorFlow checkpoint)\n",
    "import json\n",
    "\n",
    "# Read hparams\n",
    "hparams_path = parent_dir / 'gpt-2-124M_checkpoints' / 'hparams.json'\n",
    "with open(hparams_path, 'r') as f:\n",
    "    hparams = json.load(f)\n",
    "\n",
    "print(\"GPT-2 Hyperparameters:\")\n",
    "for key, value in hparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create GPT config\n",
    "gpt_config = GPTConfig(\n",
    "    context_length=hparams['n_ctx'],\n",
    "    vocab_size=hparams['n_vocab'],\n",
    "    num_layers=hparams['n_layer'],\n",
    "    embd_size=hparams['n_embd'],\n",
    "    num_heads=hparams['n_head']\n",
    ")\n",
    "\n",
    "print(\"\\n✓ GPT-2 config created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e132fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghav_sarna/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "✓ GPT-2 model loaded successfully\n",
      "  Config: 12 layers, 768 embedding size\n",
      "  Vocab size: 50257\n",
      "✓ GPT-2 model loaded successfully\n",
      "  Config: 12 layers, 768 embedding size\n",
      "  Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model from pretrained weights\n",
    "print(\"Loading GPT-2 model...\")\n",
    "gpt_model = GPT.from_pretrained('gpt2')\n",
    "gpt_model = gpt_model.to(device)\n",
    "gpt_model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "print(\"✓ GPT-2 model loaded successfully\")\n",
    "print(f\"  Config: {gpt_config.num_layers} layers, {gpt_config.embd_size} embedding size\")\n",
    "print(f\"  Vocab size: {gpt_config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6084f95",
   "metadata": {},
   "source": [
    "## Step 4: Define Helper Functions\n",
    "\n",
    "These functions will handle image preprocessing and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8678eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OCR helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# OCR Preprocessing Functions\n",
    "def preprocess_word_for_ocr(word_image, target_height=32, target_width=128):\n",
    "    \"\"\"Preprocess a word image for OCR recognition\"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(word_image.shape) == 3:\n",
    "        gray = cv2.cvtColor(word_image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = word_image\n",
    "    \n",
    "    # Apply Otsu thresholding\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Resize with padding\n",
    "    h, w = binary.shape[:2]\n",
    "    scale = min(target_height / h, target_width / w)\n",
    "    new_h, new_w = int(h * scale), int(w * scale)\n",
    "    resized = cv2.resize(binary, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Create padded image\n",
    "    padded = np.ones((target_height, target_width), dtype=np.uint8) * 255\n",
    "    y_offset = (target_height - new_h) // 2\n",
    "    x_offset = (target_width - new_w) // 2\n",
    "    padded[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized\n",
    "    \n",
    "    # Normalize\n",
    "    normalized = padded.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Add channel dimension\n",
    "    preprocessed = np.expand_dims(normalized, axis=-1)\n",
    "    \n",
    "    return preprocessed\n",
    "\n",
    "def decode_ocr_predictions(predictions, encoder):\n",
    "    \"\"\"Decode CTC predictions to text\"\"\"\n",
    "    batch_size = predictions.shape[0]\n",
    "    time_steps = predictions.shape[1]\n",
    "    \n",
    "    # Create input_length for all samples\n",
    "    input_lengths = np.full((batch_size,), time_steps, dtype=np.int32)\n",
    "    \n",
    "    # Decode using CTC\n",
    "    decoded, _ = tf.keras.backend.ctc_decode(\n",
    "        predictions,\n",
    "        input_length=input_lengths,\n",
    "        greedy=True\n",
    "    )\n",
    "    \n",
    "    # Convert to text\n",
    "    decoded_texts = []\n",
    "    decoded = decoded[0].numpy()\n",
    "    for i in range(batch_size):\n",
    "        seq = decoded[i]\n",
    "        text = encoder.decode(seq)\n",
    "        decoded_texts.append(text)\n",
    "    \n",
    "    return decoded_texts\n",
    "\n",
    "print(\"✓ OCR helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4fe8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPT-2 generation function defined\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 Generation Function\n",
    "def generate_next_words(prompt, num_predictions=3, max_new_tokens=5, temperature=1.0, top_k=50):\n",
    "    \"\"\"Generate next word predictions using GPT-2\"\"\"\n",
    "    gpt_model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    idx = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Clamp to context length\n",
    "    context_len = gpt_model.config.context_length\n",
    "    if idx.shape[1] > context_len:\n",
    "        idx = idx[:, -context_len:]\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        logits, _ = gpt_model(idx)\n",
    "        logits = logits[:, -1, :]  # Get last token logits\n",
    "        \n",
    "        if temperature != 1.0:\n",
    "            logits = logits / max(temperature, 1e-5)\n",
    "        \n",
    "        # Get top-k predictions\n",
    "        topk_vals, topk_idx = torch.topk(logits, k=min(top_k, logits.shape[-1]), dim=-1)\n",
    "        probs = F.softmax(topk_vals, dim=-1)\n",
    "        \n",
    "        # Get top N predictions\n",
    "        top_n_probs, top_n_indices = torch.topk(probs, k=min(num_predictions, probs.shape[-1]), dim=-1)\n",
    "        top_n_tokens = torch.gather(topk_idx, -1, top_n_indices)\n",
    "        \n",
    "        # Decode tokens\n",
    "        predictions = []\n",
    "        for i in range(top_n_tokens.shape[1]):\n",
    "            token_id = top_n_tokens[0, i].item()\n",
    "            token_text = tokenizer.decode([token_id])\n",
    "            prob = top_n_probs[0, i].item()\n",
    "            predictions.append((token_text, prob))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"✓ GPT-2 generation function defined\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
