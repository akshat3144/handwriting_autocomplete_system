{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f48b0f",
   "metadata": {},
   "source": [
    "# HiGAN+ Model Evaluation and Analysis\n",
    "\n",
    "This notebook provides comprehensive evaluation of the pre-trained HiGAN+ handwriting generation model using the `epoch_20.pth` checkpoint.\n",
    "\n",
    "## Evaluation Pipeline:\n",
    "1. **Load Pre-trained Model** - Restore all network components from checkpoint\n",
    "2. **Quantitative Metrics** - FID, IS, CER, WER, SSIM, PSNR\n",
    "3. **Qualitative Analysis** - Visualizations, style transfer, interpolation\n",
    "4. **Performance Report** - Comprehensive summary and comparison\n",
    "\n",
    "**Model Checkpoint:** `B:\\College\\DL\\handwriting_autocomplete_system\\Higan+ from Scratch\\server_files\\epoch_20.pth`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d154cf7",
   "metadata": {},
   "source": [
    "## 1. Load Pre-trained Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bf92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Fix for Pillow>=10\n",
    "if not hasattr(Image, \"ANTIALIAS\"):\n",
    "    Image.ANTIALIAS = Image.Resampling.LANCZOS\n",
    "\n",
    "# Add project paths\n",
    "project_path = r'B:\\College\\DL\\handwriting_autocomplete_system\\Higan+ from Scratch'\n",
    "sys.path.insert(0, project_path)\n",
    "\n",
    "# Import HiGAN+ modules\n",
    "from lib.datasets import get_dataset, get_collect_fn, Hdf5Dataset\n",
    "from lib.alphabet import strLabelConverter, Alphabets, get_lexicon, get_true_alphabet\n",
    "from lib.utils import yaml2config, draw_image, AverageMeter\n",
    "from networks import get_model\n",
    "from networks.BigGAN_networks import Generator, Discriminator, PatchDiscriminator\n",
    "from networks.module import Recognizer, WriterIdentifier, StyleEncoder, StyleBackbone\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Create output directory for results\n",
    "output_dir = Path(project_path) / \"evaluation_results\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"Results will be saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83353b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = os.path.join(project_path, 'configs', 'gan_iam.yml')\n",
    "cfg = yaml2config(config_path)\n",
    "\n",
    "# Override config settings\n",
    "cfg.device = str(device)\n",
    "cfg.training.batch_size = 16\n",
    "cfg.training.eval_batch_size = 32\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Dataset: {cfg.dataset}\")\n",
    "print(f\"  Model: {cfg.model}\")\n",
    "print(f\"  Image height: {cfg.img_height}\")\n",
    "print(f\"  Character width: {cfg.char_width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10299958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = r\"B:\\College\\DL\\handwriting_autocomplete_system\\Higan+ from Scratch\\server_files\\epoch_20.pth\"\n",
    "\n",
    "print(f\"\\nLoading checkpoint from: {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "print(\"\\nCheckpoint contents:\")\n",
    "for key in checkpoint.keys():\n",
    "    print(f\"  • {key}\")\n",
    "\n",
    "trained_epoch = checkpoint.get('epoch', 'Unknown')\n",
    "print(f\"\\nTrained Epoch: {trained_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a05767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model architectures\n",
    "print(\"\\nInitializing model architectures...\")\n",
    "\n",
    "# Generator\n",
    "generator = Generator(**cfg.GenModel).to(device)\n",
    "generator.load_state_dict(checkpoint['generator'])\n",
    "generator.eval()\n",
    "print(f\"✓ Generator loaded - Style dim: {generator.style_dim}\")\n",
    "\n",
    "# Discriminators\n",
    "discriminator = Discriminator(**cfg.DiscModel).to(device)\n",
    "discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "discriminator.eval()\n",
    "\n",
    "patch_discriminator = PatchDiscriminator(**cfg.PatchDiscModel).to(device)\n",
    "patch_discriminator.load_state_dict(checkpoint['patch_discriminator'])\n",
    "patch_discriminator.eval()\n",
    "print(f\"✓ Discriminators loaded\")\n",
    "\n",
    "# Style Encoder & Backbone\n",
    "style_backbone = StyleBackbone(**cfg.StyBackbone).to(device)\n",
    "style_encoder = StyleEncoder(**cfg.EncModel).to(device)\n",
    "style_encoder.load_state_dict(checkpoint['style_encoder'])\n",
    "style_encoder.eval()\n",
    "print(f\"✓ Style Encoder loaded\")\n",
    "\n",
    "# Recognizer (OCR)\n",
    "recognizer = Recognizer(**cfg.OcrModel).to(device)\n",
    "print(f\"✓ Recognizer initialized\")\n",
    "\n",
    "# Writer Identifier\n",
    "writer_identifier = WriterIdentifier(**cfg.WidModel).to(device)\n",
    "writer_identifier.load_state_dict(checkpoint['writer_identifier'])\n",
    "writer_identifier.eval()\n",
    "print(f\"✓ Writer Identifier loaded\")\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n--- Model Parameters ---\")\n",
    "print(f\"Generator: {count_parameters(generator):,}\")\n",
    "print(f\"Discriminator: {count_parameters(discriminator):,}\")\n",
    "print(f\"Style Encoder: {count_parameters(style_encoder):,}\")\n",
    "print(f\"Writer ID: {count_parameters(writer_identifier):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained OCR and Writer ID models\n",
    "print(\"\\nLoading pretrained auxiliary models...\")\n",
    "\n",
    "try:\n",
    "    if os.path.exists(cfg.training.pretrained_r):\n",
    "        r_dict = torch.load(cfg.training.pretrained_r, map_location=device)\n",
    "        recognizer.load_state_dict(r_dict['Recognizer'])\n",
    "        print(f\"✓ Loaded pretrained OCR from {cfg.training.pretrained_r}\")\n",
    "    else:\n",
    "        print(f\"⚠ Pretrained OCR not found: {cfg.training.pretrained_r}\")\n",
    "    \n",
    "    if os.path.exists(cfg.training.pretrained_w):\n",
    "        w_dict = torch.load(cfg.training.pretrained_w, map_location=device)\n",
    "        if 'StyleBackbone' in w_dict:\n",
    "            style_backbone.load_state_dict(w_dict['StyleBackbone'])\n",
    "        print(f\"✓ Loaded pretrained Writer ID from {cfg.training.pretrained_w}\")\n",
    "    else:\n",
    "        print(f\"⚠ Pretrained Writer ID not found: {cfg.training.pretrained_w}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error loading pretrained models: {e}\")\n",
    "\n",
    "recognizer.eval()\n",
    "print(\"\\n✓ All models loaded and set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df5ea5",
   "metadata": {},
   "source": [
    "## 2. Setup Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "print(\"Loading evaluation dataset...\")\n",
    "\n",
    "collect_fn = get_collect_fn(cfg.training.sort_input, sort_style=True)\n",
    "\n",
    "try:\n",
    "    # Load test dataset\n",
    "    test_dataset = get_dataset(\n",
    "        cfg.valid.dset_name,\n",
    "        cfg.valid.dset_split,\n",
    "        recogn_aug=False,\n",
    "        wid_aug=False,\n",
    "        process_style=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=cfg.training.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collect_fn,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Test samples: {len(test_dataset)}\")\n",
    "    print(f\"✓ Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Also load a subset of training data for comparison\n",
    "    train_dataset = get_dataset(\n",
    "        cfg.dataset,\n",
    "        cfg.training.dset_split,\n",
    "        recogn_aug=False,\n",
    "        wid_aug=False,\n",
    "        process_style=True\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.training.eval_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collect_fn,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Train samples (for reference): {len(train_dataset)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b190cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lexicon and label converter\n",
    "label_converter = strLabelConverter('all')\n",
    "\n",
    "lexicon = get_lexicon(\n",
    "    cfg.training.lexicon,\n",
    "    get_true_alphabet(cfg.dataset),\n",
    "    max_length=cfg.training.max_word_len\n",
    ")\n",
    "\n",
    "if not lexicon:\n",
    "    print(\"Building fallback lexicon from dataset...\")\n",
    "    alphabet = get_true_alphabet(cfg.dataset)\n",
    "    dataset_words = set()\n",
    "    for start_idx, length in zip(train_dataset.lb_seek_idxs, train_dataset.lb_lens):\n",
    "        length = int(length)\n",
    "        if length <= 1 or length >= cfg.training.max_word_len:\n",
    "            continue\n",
    "        raw_word = ''.join(chr(ch) for ch in train_dataset.lbs[start_idx:start_idx + length])\n",
    "        filtered_word = ''.join(ch for ch in raw_word if ch in alphabet)\n",
    "        if len(filtered_word) > 1:\n",
    "            dataset_words.add(filtered_word.lower())\n",
    "    lexicon = sorted(dataset_words)\n",
    "\n",
    "print(f\"✓ Lexicon loaded: {len(lexicon)} words\")\n",
    "print(f\"Sample words: {lexicon[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ab5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample test data\n",
    "sample_batch = next(iter(test_loader))\n",
    "\n",
    "imgs = sample_batch['style_imgs'][:8]\n",
    "lbs = sample_batch['lbs'][:8]\n",
    "lb_lens = sample_batch['lb_lens'][:8]\n",
    "texts = label_converter.decode(lbs, lb_lens)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(min(8, imgs.size(0))):\n",
    "    img = imgs[i].squeeze().numpy()\n",
    "    img = (1 - img) / 2\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f'\"{texts[i]}\"', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Test Dataset Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'sample_test_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Sample visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdad608",
   "metadata": {},
   "source": [
    "## 3. Generate Sample Images\n",
    "\n",
    "Generate diverse samples for evaluation using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c10b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.rand_dist import prepare_z_dist, prepare_y_dist\n",
    "from networks.utils import idx_to_words\n",
    "\n",
    "print(\"Setting up random distributions for sampling...\")\n",
    "\n",
    "# Initialize random distributions\n",
    "z_dist = prepare_z_dist(cfg.training.eval_batch_size, cfg.EncModel.style_dim, device, seed=42)\n",
    "y_dist = prepare_y_dist(cfg.training.eval_batch_size, len(lexicon), device, seed=42)\n",
    "\n",
    "print(f\"✓ Style distribution: {z_dist.mean().shape}\")\n",
    "print(f\"✓ Text distribution: {y_dist.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11339520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with random styles\n",
    "print(\"Generating random style samples...\")\n",
    "\n",
    "n_random_samples = 100\n",
    "random_generated_imgs = []\n",
    "random_generated_texts = []\n",
    "max_label_len = 25\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(n_random_samples // cfg.training.eval_batch_size), desc=\"Random Generation\"):\n",
    "        # Sample random texts\n",
    "        y_dist.sample_()\n",
    "        sampled_words = idx_to_words(y_dist, lexicon, max_label_len, 0.3, 0.0)\n",
    "        fake_lbs, fake_lb_lens = label_converter.encode(sampled_words, max_label_len)\n",
    "        fake_lbs = fake_lbs.to(device)\n",
    "        fake_lb_lens = fake_lb_lens.to(device)\n",
    "        \n",
    "        # Sample random styles\n",
    "        z_dist.sample_()\n",
    "        \n",
    "        # Generate images\n",
    "        fake_imgs = generator(z_dist, fake_lbs, fake_lb_lens)\n",
    "        \n",
    "        random_generated_imgs.append(fake_imgs.cpu())\n",
    "        random_generated_texts.extend(sampled_words)\n",
    "\n",
    "random_generated_imgs = torch.cat(random_generated_imgs, dim=0)\n",
    "print(f\"✓ Generated {random_generated_imgs.size(0)} random style images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d24f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate style-guided samples from test set\n",
    "print(\"Generating style-guided samples...\")\n",
    "\n",
    "style_guided_imgs = []\n",
    "style_guided_texts = []\n",
    "reference_imgs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Style-Guided Generation\")):\n",
    "        if batch_idx >= 5:  # Limit to 5 batches\n",
    "            break\n",
    "            \n",
    "        real_imgs = batch['style_imgs'].to(device)\n",
    "        real_img_lens = batch['style_img_lens'].to(device)\n",
    "        real_lbs = batch['lbs'].to(device)\n",
    "        real_lb_lens = batch['lb_lens'].to(device)\n",
    "        \n",
    "        # Extract style\n",
    "        enc_z = style_encoder(real_imgs, real_img_lens, style_backbone, vae_mode=False)\n",
    "        \n",
    "        # Generate with same content\n",
    "        fake_imgs = generator(enc_z, real_lbs, real_lb_lens)\n",
    "        \n",
    "        style_guided_imgs.append(fake_imgs.cpu())\n",
    "        reference_imgs.append(real_imgs.cpu())\n",
    "        texts = label_converter.decode(real_lbs, real_lb_lens)\n",
    "        style_guided_texts.extend(texts)\n",
    "\n",
    "style_guided_imgs = torch.cat(style_guided_imgs, dim=0)\n",
    "reference_imgs = torch.cat(reference_imgs, dim=0)\n",
    "print(f\"✓ Generated {style_guided_imgs.size(0)} style-guided images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b4a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fixed text with varying styles\n",
    "print(\"Generating fixed texts with varying styles...\")\n",
    "\n",
    "fixed_texts = [\"Hello\", \"World\", \"Python\", \"Deep\", \"Learning\", \"Neural\", \"Network\", \"Artificial\"]\n",
    "fixed_text_imgs = []\n",
    "\n",
    "n_style_variations = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(fixed_texts, desc=\"Fixed Text Generation\"):\n",
    "        text_imgs = []\n",
    "        \n",
    "        for _ in range(n_style_variations):\n",
    "            # Random style\n",
    "            z = torch.randn(1, generator.style_dim).to(device)\n",
    "            \n",
    "            # Encode text\n",
    "            lbs, lb_lens = label_converter.encode([text], max_label_len)\n",
    "            lbs = lbs.to(device)\n",
    "            lb_lens = lb_lens.to(device)\n",
    "            \n",
    "            # Generate\n",
    "            fake_img = generator(z, lbs, lb_lens)\n",
    "            text_imgs.append(fake_img.cpu())\n",
    "        \n",
    "        fixed_text_imgs.append(torch.cat(text_imgs, dim=0))\n",
    "\n",
    "print(f\"✓ Generated {len(fixed_texts)} texts with {n_style_variations} style variations each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d0c00a",
   "metadata": {},
   "source": [
    "## 4. Calculate FID (Fréchet Inception Distance)\n",
    "\n",
    "FID measures the quality and diversity of generated images by comparing feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51771e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch-fid if not available\n",
    "try:\n",
    "    from pytorch_fid import fid_score\n",
    "    from pytorch_fid.inception import InceptionV3\n",
    "    print(\"✓ pytorch-fid available\")\n",
    "except ImportError:\n",
    "    print(\"Installing pytorch-fid...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytorch-fid\"])\n",
    "    from pytorch_fid import fid_score\n",
    "    from pytorch_fid.inception import InceptionV3\n",
    "    print(\"✓ pytorch-fid installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def calculate_activation_statistics(imgs, model, batch_size=50, dims=2048, device='cpu'):\n",
    "    \"\"\"Calculate mean and covariance statistics of inception features\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare images (convert grayscale to RGB)\n",
    "    if imgs.size(1) == 1:\n",
    "        imgs = imgs.repeat(1, 3, 1, 1)\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    imgs = (imgs - 0.5) / 0.5\n",
    "    \n",
    "    n_samples = imgs.size(0)\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    pred_arr = np.empty((n_samples, dims))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_batches), desc=\"Extracting features\"):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch = imgs[start:end].to(device)\n",
    "            \n",
    "            # Resize to 299x299 for InceptionV3\n",
    "            batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            pred = model(batch)[0]\n",
    "            \n",
    "            if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "                pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "            \n",
    "            pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "            pred_arr[start:end] = pred\n",
    "    \n",
    "    mu = np.mean(pred_arr, axis=0)\n",
    "    sigma = np.cov(pred_arr, rowvar=False)\n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Calculate Fréchet Distance between two Gaussian distributions\"\"\"\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "    # Product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "    \n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(f'Imaginary component {m}')\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    tr_covmean = np.trace(covmean)\n",
    "    \n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "print(\"Setting up Inception V3 model for FID calculation...\")\n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "inception_model = InceptionV3([block_idx]).to(device)\n",
    "inception_model.eval()\n",
    "print(\"✓ Inception V3 ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efa021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate FID score\n",
    "print(\"\\nCalculating FID score...\")\n",
    "\n",
    "# Collect real images\n",
    "real_imgs_for_fid = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Collecting real images\")):\n",
    "        if batch_idx >= 10:  # Limit samples\n",
    "            break\n",
    "        real_imgs_for_fid.append(batch['style_imgs'])\n",
    "\n",
    "real_imgs_for_fid = torch.cat(real_imgs_for_fid, dim=0)\n",
    "print(f\"Real images for FID: {real_imgs_for_fid.shape}\")\n",
    "\n",
    "# Use generated images\n",
    "fake_imgs_for_fid = style_guided_imgs[:real_imgs_for_fid.size(0)]\n",
    "print(f\"Fake images for FID: {fake_imgs_for_fid.shape}\")\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"\\nCalculating real image statistics...\")\n",
    "mu_real, sigma_real = calculate_activation_statistics(\n",
    "    real_imgs_for_fid, inception_model, batch_size=32, device=device\n",
    ")\n",
    "\n",
    "print(\"Calculating generated image statistics...\")\n",
    "mu_fake, sigma_fake = calculate_activation_statistics(\n",
    "    fake_imgs_for_fid, inception_model, batch_size=32, device=device\n",
    ")\n",
    "\n",
    "# Calculate FID\n",
    "fid_value = calculate_frechet_distance(mu_real, sigma_real, mu_fake, sigma_fake)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FID Score: {fid_value:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"(Lower is better. FID < 50 is good, FID < 20 is excellent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecbd3c2",
   "metadata": {},
   "source": [
    "## 5. Calculate Inception Score (IS)\n",
    "\n",
    "IS measures both quality and diversity of generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a1735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3\n",
    "\n",
    "def calculate_inception_score(imgs, model, batch_size=32, splits=10, device='cpu'):\n",
    "    \"\"\"Calculate Inception Score\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare images\n",
    "    if imgs.size(1) == 1:\n",
    "        imgs = imgs.repeat(1, 3, 1, 1)\n",
    "    \n",
    "    imgs = (imgs - 0.5) / 0.5\n",
    "    \n",
    "    n_samples = imgs.size(0)\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_batches), desc=\"Computing IS\"):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch = imgs[start:end].to(device)\n",
    "            \n",
    "            # Resize to 299x299\n",
    "            batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            pred = model(batch)\n",
    "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
    "            preds.append(pred)\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    \n",
    "    # Calculate score\n",
    "    split_scores = []\n",
    "    \n",
    "    for k in range(splits):\n",
    "        part = preds[k * (n_samples // splits): (k + 1) * (n_samples // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(np.sum(pyx * np.log(pyx / py + 1e-10)))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "    \n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "print(\"Loading Inception V3 for IS calculation...\")\n",
    "inception_v3_model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception_v3_model.eval()\n",
    "print(\"✓ Inception V3 loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a137cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Inception Score\n",
    "print(\"\\nCalculating Inception Score...\")\n",
    "\n",
    "is_imgs = random_generated_imgs[:500] if random_generated_imgs.size(0) > 500 else random_generated_imgs\n",
    "\n",
    "is_mean, is_std = calculate_inception_score(\n",
    "    is_imgs, inception_v3_model, batch_size=32, splits=10, device=device\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Inception Score: {is_mean:.4f} ± {is_std:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"(Higher is better. IS > 5 is good, IS > 10 is excellent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203a68c",
   "metadata": {},
   "source": [
    "## 6. Calculate OCR Metrics (CER & WER)\n",
    "\n",
    "Evaluate text readability using Character Error Rate and Word Error Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distance import levenshtein\n",
    "from networks.utils import ctc_greedy_decoder\n",
    "\n",
    "def evaluate_ocr_metrics(model_rec, generated_imgs, generated_labels, generated_label_lens, \n",
    "                        label_conv, ctc_len_scale, char_width, device):\n",
    "    \"\"\"Calculate CER and WER for generated images\"\"\"\n",
    "    model_rec.eval()\n",
    "    \n",
    "    char_errors = 0\n",
    "    total_chars = 0\n",
    "    word_errors = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process in batches\n",
    "        batch_size = 32\n",
    "        n_batches = (generated_imgs.size(0) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in tqdm(range(n_batches), desc=\"Running OCR\"):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, generated_imgs.size(0))\n",
    "            \n",
    "            imgs = generated_imgs[start:end].to(device)\n",
    "            lbs = generated_labels[start:end]\n",
    "            lb_lens = generated_label_lens[start:end]\n",
    "            \n",
    "            img_lens = lb_lens * char_width\n",
    "            \n",
    "            # Run OCR\n",
    "            logits = model_rec(imgs, img_lens.to(device))\n",
    "            logits = F.softmax(logits, dim=2).cpu().numpy()\n",
    "            \n",
    "            # Decode predictions\n",
    "            for logit, img_len in zip(logits, img_lens.cpu().numpy()):\n",
    "                label = ctc_greedy_decoder(logit[:img_len // ctc_len_scale])\n",
    "                predictions.append(label_conv.decode(label))\n",
    "            \n",
    "            # Ground truth\n",
    "            ground_truths.extend(label_conv.decode(lbs, lb_lens))\n",
    "    \n",
    "    # Calculate errors\n",
    "    for pred, gt in zip(predictions, ground_truths):\n",
    "        char_error = levenshtein(pred, gt)\n",
    "        char_errors += char_error\n",
    "        total_chars += len(gt)\n",
    "        total_words += 1\n",
    "        if char_error > 0:\n",
    "            word_errors += 1\n",
    "    \n",
    "    cer = char_errors / total_chars if total_chars > 0 else 0\n",
    "    wer = word_errors / total_words if total_words > 0 else 0\n",
    "    \n",
    "    return cer, wer, predictions, ground_truths\n",
    "\n",
    "print(\"Preparing for OCR evaluation...\")\n",
    "ctc_len_scale = recognizer.len_scale\n",
    "print(f\"CTC length scale: {ctc_len_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate OCR on style-guided samples\n",
    "print(\"\\nEvaluating OCR on style-guided samples...\")\n",
    "\n",
    "# Prepare labels for style-guided images\n",
    "style_guided_lbs = []\n",
    "style_guided_lb_lens = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        if batch_idx >= 5:\n",
    "            break\n",
    "        style_guided_lbs.append(batch['lbs'])\n",
    "        style_guided_lb_lens.append(batch['lb_lens'])\n",
    "\n",
    "style_guided_lbs = torch.cat(style_guided_lbs, dim=0)\n",
    "style_guided_lb_lens = torch.cat(style_guided_lb_lens, dim=0)\n",
    "\n",
    "cer_style, wer_style, preds_style, gts_style = evaluate_ocr_metrics(\n",
    "    recognizer, \n",
    "    style_guided_imgs,\n",
    "    style_guided_lbs,\n",
    "    style_guided_lb_lens,\n",
    "    label_converter,\n",
    "    ctc_len_scale,\n",
    "    cfg.char_width,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OCR Metrics (Style-Guided Generation):\")\n",
    "print(f\"  Character Error Rate (CER): {cer_style*100:.2f}%\")\n",
    "print(f\"  Word Error Rate (WER): {wer_style*100:.2f}%\")\n",
    "print(f\"  Character Accuracy: {(1-cer_style)*100:.2f}%\")\n",
    "print(f\"  Word Accuracy: {(1-wer_style)*100:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f282b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some OCR predictions vs ground truth\n",
    "print(\"\\nSample OCR Predictions vs Ground Truth:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "n_samples = min(20, len(preds_style))\n",
    "correct = 0\n",
    "\n",
    "for i in range(n_samples):\n",
    "    match = \"✓\" if preds_style[i] == gts_style[i] else \"✗\"\n",
    "    if preds_style[i] == gts_style[i]:\n",
    "        correct += 1\n",
    "    print(f\"{match} GT: '{gts_style[i]:15s}' | Pred: '{preds_style[i]:15s}'\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Accuracy in sample: {correct}/{n_samples} ({100*correct/n_samples:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8f2cc",
   "metadata": {},
   "source": [
    "## 7. Calculate Writer Identification Accuracy\n",
    "\n",
    "Evaluate how well the model preserves writer-specific characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_writer_identification(model_wid, model_backbone, generated_imgs, \n",
    "                                   generated_lb_lens, true_wids, char_width, device):\n",
    "    \"\"\"Calculate writer identification accuracy\"\"\"\n",
    "    model_wid.eval()\n",
    "    model_backbone.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_size = 32\n",
    "        n_batches = (generated_imgs.size(0) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in tqdm(range(n_batches), desc=\"Writer ID Evaluation\"):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, generated_imgs.size(0))\n",
    "            \n",
    "            imgs = generated_imgs[start:end].to(device)\n",
    "            lb_lens = generated_lb_lens[start:end]\n",
    "            wids = true_wids[start:end]\n",
    "            \n",
    "            img_lens = lb_lens * char_width\n",
    "            \n",
    "            # Get writer predictions\n",
    "            logits = model_wid(imgs, img_lens.to(device), model_backbone)\n",
    "            preds = torch.argmax(logits, dim=1).cpu()\n",
    "            \n",
    "            correct += (preds == wids).sum().item()\n",
    "            total += imgs.size(0)\n",
    "            \n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_true.extend(wids.tolist())\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    return accuracy, all_preds, all_true\n",
    "\n",
    "print(\"Evaluating writer identification accuracy...\")\n",
    "\n",
    "# Get writer IDs for style-guided samples\n",
    "style_guided_wids = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        if batch_idx >= 5:\n",
    "            break\n",
    "        style_guided_wids.append(batch['wids'])\n",
    "\n",
    "style_guided_wids = torch.cat(style_guided_wids, dim=0)\n",
    "\n",
    "wid_accuracy, wid_preds, wid_true = evaluate_writer_identification(\n",
    "    writer_identifier,\n",
    "    style_backbone,\n",
    "    style_guided_imgs,\n",
    "    style_guided_lb_lens,\n",
    "    style_guided_wids,\n",
    "    cfg.char_width,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Writer Identification Accuracy: {wid_accuracy*100:.2f}%\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total samples evaluated: {len(wid_preds)}\")\n",
    "print(f\"Correct predictions: {sum(1 for p, t in zip(wid_preds, wid_true) if p == t)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0dee25",
   "metadata": {},
   "source": [
    "## 8. Calculate Image Quality Metrics (SSIM, PSNR)\n",
    "\n",
    "Compute structural similarity and peak signal-to-noise ratio for reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "def calculate_image_quality_metrics(real_imgs, generated_imgs):\n",
    "    \"\"\"Calculate SSIM and PSNR between real and generated images\"\"\"\n",
    "    ssim_scores = []\n",
    "    psnr_scores = []\n",
    "    \n",
    "    n_samples = min(real_imgs.size(0), generated_imgs.size(0))\n",
    "    \n",
    "    for i in tqdm(range(n_samples), desc=\"Calculating SSIM/PSNR\"):\n",
    "        # Convert to numpy\n",
    "        real_img = real_imgs[i].squeeze().cpu().numpy()\n",
    "        gen_img = generated_imgs[i].squeeze().cpu().numpy()\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        real_img = (real_img + 1) / 2\n",
    "        gen_img = (gen_img + 1) / 2\n",
    "        \n",
    "        # Calculate SSIM\n",
    "        ssim_val = ssim(real_img, gen_img, data_range=1.0)\n",
    "        ssim_scores.append(ssim_val)\n",
    "        \n",
    "        # Calculate PSNR\n",
    "        psnr_val = psnr(real_img, gen_img, data_range=1.0)\n",
    "        psnr_scores.append(psnr_val)\n",
    "    \n",
    "    return np.mean(ssim_scores), np.std(ssim_scores), np.mean(psnr_scores), np.std(psnr_scores)\n",
    "\n",
    "print(\"Calculating SSIM and PSNR...\")\n",
    "\n",
    "ssim_mean, ssim_std, psnr_mean, psnr_std = calculate_image_quality_metrics(\n",
    "    reference_imgs, style_guided_imgs\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Image Quality Metrics:\")\n",
    "print(f\"  SSIM: {ssim_mean:.4f} ± {ssim_std:.4f}\")\n",
    "print(f\"  PSNR: {psnr_mean:.2f} ± {psnr_std:.2f} dB\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"SSIM: 1.0 is perfect, > 0.7 is good\")\n",
    "print(\"PSNR: Higher is better, > 30 dB is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273d1bd",
   "metadata": {},
   "source": [
    "## 9. Style Consistency Analysis\n",
    "\n",
    "Measure how consistently the model maintains a given writing style across different texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fdb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_style_features(model_encoder, model_backbone, imgs, img_lens, device):\n",
    "    \"\"\"Extract style features from images\"\"\"\n",
    "    model_encoder.eval()\n",
    "    model_backbone.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        imgs = imgs.to(device)\n",
    "        img_lens = img_lens.to(device)\n",
    "        features = model_encoder(imgs, img_lens, model_backbone, vae_mode=False)\n",
    "    \n",
    "    return features.cpu()\n",
    "\n",
    "def calculate_style_consistency(features):\n",
    "    \"\"\"Calculate variance in style features (lower = more consistent)\"\"\"\n",
    "    # Calculate pairwise distances\n",
    "    features_np = features.numpy()\n",
    "    \n",
    "    # Mean feature vector\n",
    "    mean_feat = np.mean(features_np, axis=0)\n",
    "    \n",
    "    # Calculate variance\n",
    "    variance = np.mean(np.var(features_np, axis=0))\n",
    "    \n",
    "    # Calculate pairwise cosine similarity\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(features_np)\n",
    "    \n",
    "    # Get upper triangle (excluding diagonal)\n",
    "    n = similarities.shape[0]\n",
    "    upper_tri_idx = np.triu_indices(n, k=1)\n",
    "    pairwise_sims = similarities[upper_tri_idx]\n",
    "    \n",
    "    mean_similarity = np.mean(pairwise_sims)\n",
    "    std_similarity = np.std(pairwise_sims)\n",
    "    \n",
    "    return variance, mean_similarity, std_similarity\n",
    "\n",
    "print(\"Analyzing style consistency...\")\n",
    "\n",
    "# For each text, extract style features from all variations\n",
    "consistency_results = []\n",
    "\n",
    "for text_idx, text in enumerate(tqdm(fixed_texts, desc=\"Style Consistency\")):\n",
    "    imgs = fixed_text_imgs[text_idx]\n",
    "    img_lens = torch.ones(imgs.size(0), dtype=torch.int) * imgs.size(3)\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_style_features(style_encoder, style_backbone, imgs, img_lens, device)\n",
    "    \n",
    "    # Calculate consistency\n",
    "    variance, mean_sim, std_sim = calculate_style_consistency(features)\n",
    "    \n",
    "    consistency_results.append({\n",
    "        'text': text,\n",
    "        'variance': variance,\n",
    "        'mean_similarity': mean_sim,\n",
    "        'std_similarity': std_sim\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Style Consistency Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for result in consistency_results:\n",
    "    print(f\"Text: '{result['text']:10s}' | Variance: {result['variance']:.4f} | \"\n",
    "          f\"Similarity: {result['mean_similarity']:.4f}±{result['std_similarity']:.4f}\")\n",
    "\n",
    "avg_variance = np.mean([r['variance'] for r in consistency_results])\n",
    "avg_similarity = np.mean([r['mean_similarity'] for r in consistency_results])\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Average Variance: {avg_variance:.4f} (lower = more consistent)\")\n",
    "print(f\"Average Similarity: {avg_similarity:.4f} (higher = more consistent)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4493df",
   "metadata": {},
   "source": [
    "## 10. Diversity Metrics\n",
    "\n",
    "Evaluate output diversity to ensure the model doesn't suffer from mode collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ce9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity_metrics(imgs):\n",
    "    \"\"\"Calculate diversity metrics for generated images\"\"\"\n",
    "    imgs_np = imgs.numpy()\n",
    "    \n",
    "    # Reshape to (n_samples, -1)\n",
    "    n_samples = imgs_np.shape[0]\n",
    "    imgs_flat = imgs_np.reshape(n_samples, -1)\n",
    "    \n",
    "    # Calculate pairwise L2 distances\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    distances = euclidean_distances(imgs_flat)\n",
    "    \n",
    "    # Get upper triangle\n",
    "    upper_tri_idx = np.triu_indices(n_samples, k=1)\n",
    "    pairwise_dists = distances[upper_tri_idx]\n",
    "    \n",
    "    mean_dist = np.mean(pairwise_dists)\n",
    "    std_dist = np.std(pairwise_dists)\n",
    "    \n",
    "    # Calculate entropy of pixel values\n",
    "    pixel_values = imgs_flat.flatten()\n",
    "    hist, _ = np.histogram(pixel_values, bins=50, density=True)\n",
    "    hist = hist[hist > 0]\n",
    "    entropy = -np.sum(hist * np.log(hist + 1e-10))\n",
    "    \n",
    "    return mean_dist, std_dist, entropy\n",
    "\n",
    "print(\"Calculating diversity metrics...\")\n",
    "\n",
    "# Intra-class diversity (same style, different texts)\n",
    "intra_diversities = []\n",
    "for text_idx in range(len(fixed_texts)):\n",
    "    imgs = fixed_text_imgs[text_idx]\n",
    "    mean_dist, std_dist, entropy = calculate_diversity_metrics(imgs)\n",
    "    intra_diversities.append({\n",
    "        'text': fixed_texts[text_idx],\n",
    "        'mean_dist': mean_dist,\n",
    "        'std_dist': std_dist,\n",
    "        'entropy': entropy\n",
    "    })\n",
    "\n",
    "# Inter-class diversity (different styles)\n",
    "inter_mean_dist, inter_std_dist, inter_entropy = calculate_diversity_metrics(random_generated_imgs[:100])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Diversity Metrics:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nIntra-class Diversity (same style, different cannot-vary texts):\")\n",
    "for result in intra_diversities:\n",
    "    print(f\"  Text '{result['text']:10s}': Distance={result['mean_dist']:.4f}±{result['std_dist']:.4f}, \"\n",
    "          f\"Entropy={result['entropy']:.4f}\")\n",
    "\n",
    "avg_intra_dist = np.mean([r['mean_dist'] for r in intra_diversities])\n",
    "avg_intra_entropy = np.mean([r['entropy'] for r in intra_diversities])\n",
    "\n",
    "print(f\"\\nAverage Intra-class Distance: {avg_intra_dist:.4f}\")\n",
    "print(f\"Average Intra-class Entropy: {avg_intra_entropy:.4f}\")\n",
    "\n",
    "print(f\"\\nInter-class Diversity (different random styles):\")\n",
    "print(f\"  Mean Distance: {inter_mean_dist:.4f}±{inter_std_dist:.4f}\")\n",
    "print(f\"  Entropy: {inter_entropy:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Diversity Ratio (Inter/Intra): {inter_mean_dist / avg_intra_dist:.2f}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"(Higher ratio indicates better diversity across different styles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e92c92",
   "metadata": {},
   "source": [
    "## 11. Qualitative Visualizations - Style Transfer\n",
    "\n",
    "Visualize reference images, reconstructions, and style-transferred versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5755077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive style transfer visualization\n",
    "n_examples = 8\n",
    "\n",
    "fig, axes = plt.subplots(n_examples, 3, figsize=(15, 3*n_examples))\n",
    "\n",
    "for i in range(n_examples):\n",
    "    # Reference image\n",
    "    ref_img = (1 - reference_imgs[i].squeeze().numpy())\n",
    "    axes[i, 0].imshow(ref_img, cmap='gray')\n",
    "    axes[i, 0].set_title(f'Reference: \"{style_guided_texts[i]}\"', fontsize=10)\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Reconstruction\n",
    "    recon_img = (1 - style_guided_imgs[i].squeeze().numpy())\n",
    "    axes[i, 1].imshow(recon_img, cmap='gray')\n",
    "    axes[i, 1].set_title(f'Reconstruction', fontsize=10)\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # OCR prediction\n",
    "    pred_text = preds_style[i] if i < len(preds_style) else \"\"\n",
    "    gt_text = gts_style[i] if i < len(gts_style) else \"\"\n",
    "    match = \"✓\" if pred_text == gt_text else \"✗\"\n",
    "    \n",
    "    axes[i, 2].text(0.5, 0.5, f'{match}\\nGT: \"{gt_text}\"\\nPred: \"{pred_text}\"',\n",
    "                    ha='center', va='center', fontsize=9, \n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Style Transfer: Reference → Reconstruction → OCR Verification', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'visualization_style_transfer.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Style transfer visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a475b",
   "metadata": {},
   "source": [
    "## 12. Qualitative Visualizations - Random Generation\n",
    "\n",
    "Display a grid of randomly generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ca9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random generation grid\n",
    "n_rows = 4\n",
    "n_cols = 6\n",
    "n_total = n_rows * n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(n_total):\n",
    "    if i < random_generated_imgs.size(0):\n",
    "        img = (1 - random_generated_imgs[i].squeeze().numpy())\n",
    "        text = random_generated_texts[i] if i < len(random_generated_texts) else \"\"\n",
    "        \n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'\"{text}\"', fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Random Style Generation - Diverse Samples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'visualization_random_generation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Random generation visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a8b5e",
   "metadata": {},
   "source": [
    "## 13. Qualitative Visualizations - Style Interpolation\n",
    "\n",
    "Show smooth transitions between different writing styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a72469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate style interpolation sequences\n",
    "interpolation_text = \"Interpolate\"\n",
    "n_interpolations = 3\n",
    "n_steps = 8\n",
    "\n",
    "fig, axes = plt.subplots(n_interpolations, n_steps, figsize=(20, 3*n_interpolations))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for interp_idx in range(n_interpolations):\n",
    "        # Two random styles\n",
    "        style_a = torch.randn(1, generator.style_dim).to(device)\n",
    "        style_b = torch.randn(1, generator.style_dim).to(device)\n",
    "        \n",
    "        # Encode text\n",
    "        text_lbs, text_lb_lens = label_converter.encode([interpolation_text], 25)\n",
    "        text_lbs = text_lbs.to(device)\n",
    "        text_lb_lens = text_lb_lens.to(device)\n",
    "        \n",
    "        for step_idx in range(n_steps):\n",
    "            alpha = step_idx / (n_steps - 1)\n",
    "            style_interp = (1 - alpha) * style_a + alpha * style_b\n",
    "            \n",
    "            gen_img = generator(style_interp, text_lbs, text_lb_lens)\n",
    "            \n",
    "            img_np = (1 - gen_img.squeeze().cpu().numpy())\n",
    "            axes[interp_idx, step_idx].imshow(img_np, cmap='gray')\n",
    "            axes[interp_idx, step_idx].set_title(f'α={alpha:.2f}', fontsize=8)\n",
    "            axes[interp_idx, step_idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Style Interpolation: \"{interpolation_text}\"', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'visualization_interpolation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Style interpolation visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc68f46f",
   "metadata": {},
   "source": [
    "## 14. Qualitative Visualizations - Text Variation\n",
    "\n",
    "Show how the same style renders different text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate same style, different texts\n",
    "varied_texts = [\"Hello\", \"World\", \"123\", \"CAPS\", \"lower\", \"Mixed\", \"Special!\", \"PyTorch\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # One consistent style\n",
    "    consistent_style = torch.randn(1, generator.style_dim).to(device)\n",
    "    \n",
    "    for i, text in enumerate(varied_texts):\n",
    "        # Encode text\n",
    "        lbs, lb_lens = label_converter.encode([text], 25)\n",
    "        lbs = lbs.to(device)\n",
    "        lb_lens = lb_lens.to(device)\n",
    "        \n",
    "        # Generate\n",
    "        gen_img = generator(consistent_style, lbs, lb_lens)\n",
    "        \n",
    "        img_np = (1 - gen_img.squeeze().cpu().numpy())\n",
    "        axes[i].imshow(img_np, cmap='gray')\n",
    "        axes[i].set_title(f'\"{text}\"', fontsize=11, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Same Writing Style - Different Text Content', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'visualization_text_variation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Text variation visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bba4d9",
   "metadata": {},
   "source": [
    "## 15. Cross-Writer Style Transfer Evaluation\n",
    "\n",
    "Transfer multiple writer styles to the same text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-writer style transfer\n",
    "target_text = \"Transfer\"\n",
    "n_writers = 8\n",
    "\n",
    "# Get different writer samples from test set\n",
    "writer_samples = []\n",
    "writer_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        for i in range(batch['style_imgs'].size(0)):\n",
    "            wid = batch['wids'][i].item()\n",
    "            if wid not in writer_ids and len(writer_ids) < n_writers:\n",
    "                writer_ids.append(wid)\n",
    "                writer_samples.append({\n",
    "                    'img': batch['style_imgs'][i:i+1],\n",
    "                    'img_len': batch['style_img_lens'][i:i+1],\n",
    "                    'wid': wid,\n",
    "                    'text': label_converter.decode(batch['lbs'][i:i+1], batch['lb_lens'][i:i+1])[0]\n",
    "                })\n",
    "        \n",
    "        if len(writer_ids) >= n_writers:\n",
    "            break\n",
    "\n",
    "# Generate target text in each writer's style\n",
    "fig, axes = plt.subplots(n_writers, 3, figsize=(12, 3*n_writers))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode target text\n",
    "    target_lbs, target_lb_lens = label_converter.encode([target_text], 25)\n",
    "    target_lbs = target_lbs.to(device)\n",
    "    target_lb_lens = target_lb_lens.to(device)\n",
    "    \n",
    "    for i, sample in enumerate(writer_samples):\n",
    "        # Reference image\n",
    "        ref_img = (1 - sample['img'].squeeze().numpy())\n",
    "        axes[i, 0].imshow(ref_img, cmap='gray')\n",
    "        axes[i, 0].set_title(f'Writer {sample[\"wid\"]}\\n\"{sample[\"text\"]}\"', fontsize=9)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Extract style\n",
    "        style = style_encoder(sample['img'].to(device), sample['img_len'].to(device), \n",
    "                             style_backbone, vae_mode=False)\n",
    "        \n",
    "        # Generate target text\n",
    "        gen_img = generator(style, target_lbs, target_lb_lens)\n",
    "        gen_img_np = (1 - gen_img.squeeze().cpu().numpy())\n",
    "        axes[i, 1].imshow(gen_img_np, cmap='gray')\n",
    "        axes[i, 1].set_title(f'Generated\\n\"{target_text}\"', fontsize=9)\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Info\n",
    "        axes[i, 2].text(0.5, 0.5, f'Writer ID: {sample[\"wid\"]}\\nStyle → \"{target_text}\"',\n",
    "                       ha='center', va='center', fontsize=9,\n",
    "                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle(f'Cross-Writer Style Transfer: Multiple Writers → \"{target_text}\"', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'visualization_cross_writer.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Cross-writer style transfer visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b96e66",
   "metadata": {},
   "source": [
    "## 16. Attention Map Visualization\n",
    "\n",
    "Visualize feature maps from the Style Encoder to understand what contributes to style extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f30b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def extract_feature_maps(model_backbone, img, img_len):\n",
    "    \"\"\"Extract intermediate feature maps from style backbone\"\"\"\n",
    "    model_backbone.eval()\n",
    "    \n",
    "    feature_maps = []\n",
    "    \n",
    "    # Hook to capture intermediate outputs\n",
    "    def hook_fn(module, input, output):\n",
    "        feature_maps.append(output.detach().cpu())\n",
    "    \n",
    "    # Register hooks on convolutional layers\n",
    "    hooks = []\n",
    "    for name, module in model_backbone.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            hooks.append(module.register_forward_hook(hook_fn))\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model_backbone(img.to(device))\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return feature_maps\n",
    "\n",
    "# Visualize feature maps for sample images\n",
    "n_samples = 3\n",
    "sample_batch = next(iter(test_loader))\n",
    "\n",
    "fig = plt.figure(figsize=(18, 4*n_samples))\n",
    "\n",
    "for sample_idx in range(n_samples):\n",
    "    img = sample_batch['style_imgs'][sample_idx:sample_idx+1]\n",
    "    img_len = sample_batch['style_img_lens'][sample_idx:sample_idx+1]\n",
    "    text = label_converter.decode(sample_batch['lbs'][sample_idx:sample_idx+1], \n",
    "                                  sample_batch['lb_lens'][sample_idx:sample_idx+1])[0]\n",
    "    \n",
    "    # Extract feature maps\n",
    "    feature_maps = extract_feature_maps(style_backbone, img, img_len)\n",
    "    \n",
    "    # Select a few layers to visualize\n",
    "    n_layers = min(5, len(feature_maps))\n",
    "    layer_indices = np.linspace(0, len(feature_maps)-1, n_layers, dtype=int)\n",
    "    \n",
    "    for i, layer_idx in enumerate(layer_indices):\n",
    "        feat_map = feature_maps[layer_idx][0]  # First sample in batch\n",
    "        \n",
    "        # Average across channels\n",
    "        feat_map_avg = torch.mean(feat_map, dim=0).numpy()\n",
    "        \n",
    "        ax = plt.subplot(n_samples, n_layers + 1, sample_idx * (n_layers + 1) + i + 1)\n",
    "        \n",
    "        if i == 0:\n",
    "            # Show original image\n",
    "            orig_img = (1 - img.squeeze().numpy())\n",
    "            ax.imshow(orig_img, cmap='gray')\n",
    "            ax.set_title(f'Input: \"{text}\"', fontsize=9)\n",
    "        else:\n",
    "            # Show feature map\n",
    "            im = ax.imshow(feat_map_avg, cmap='viridis')\n",
    "            ax.set_title(f'Layer {layer_idx}', fontsize=8)\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "        \n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Style Encoder Feature Maps Visualization', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'visualization_feature_maps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Feature maps visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e6011",
   "metadata": {},
   "source": [
    "## 17. Failure Case Analysis\n",
    "\n",
    "Identify and analyze failure modes of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze failures based on OCR errors\n",
    "print(\"Analyzing failure cases...\")\n",
    "\n",
    "# Calculate CER for each sample\n",
    "individual_errors = []\n",
    "for i, (pred, gt) in enumerate(zip(preds_style, gts_style)):\n",
    "    char_error = levenshtein(pred, gt)\n",
    "    cer_individual = char_error / len(gt) if len(gt) > 0 else 0\n",
    "    \n",
    "    individual_errors.append({\n",
    "        'index': i,\n",
    "        'gt': gt,\n",
    "        'pred': pred,\n",
    "        'cer': cer_individual,\n",
    "        'char_error': char_error,\n",
    "        'word_length': len(gt)\n",
    "    })\n",
    "\n",
    "# Sort by CER\n",
    "individual_errors.sort(key=lambda x: x['cer'], reverse=True)\n",
    "\n",
    "# Get worst cases\n",
    "n_worst = 12\n",
    "worst_cases = individual_errors[:n_worst]\n",
    "\n",
    "print(f\"\\nTop {n_worst} Failure Cases (Highest CER):\")\n",
    "print(\"-\" * 80)\n",
    "for i, case in enumerate(worst_cases, 1):\n",
    "    print(f\"{i:2d}. GT: '{case['gt']:15s}' | Pred: '{case['pred']:15s}' | \"\n",
    "          f\"CER: {case['cer']*100:.1f}% | Len: {case['word_length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize failure cases\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, case in enumerate(worst_cases):\n",
    "    idx = case['index']\n",
    "    \n",
    "    if idx < style_guided_imgs.size(0):\n",
    "        img = (1 - style_guided_imgs[idx].squeeze().numpy())\n",
    "        \n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"GT: '{case['gt']}'\\nPred: '{case['pred']}'\\nCER: {case['cer']*100:.1f}%\",\n",
    "                         fontsize=9, color='red')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Failure Case Analysis - Highest OCR Errors', fontsize=14, fontweight='bold', color='red')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'analysis_failure_cases.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Failure case visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb69e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Error Pattern Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# By word length\n",
    "length_bins = [0, 3, 5, 7, 10, 100]\n",
    "length_errors = {f'{length_bins[i]}-{length_bins[i+1]}': [] \n",
    "                 for i in range(len(length_bins)-1)}\n",
    "\n",
    "for case in individual_errors:\n",
    "    for i in range(len(length_bins)-1):\n",
    "        if length_bins[i] <= case['word_length'] < length_bins[i+1]:\n",
    "            length_errors[f'{length_bins[i]}-{length_bins[i+1]}'].append(case['cer'])\n",
    "            break\n",
    "\n",
    "print(\"\\nCER by Word Length:\")\n",
    "for length_range, cers in length_errors.items():\n",
    "    if cers:\n",
    "        print(f\"  {length_range:8s}: {np.mean(cers)*100:.2f}% ± {np.std(cers)*100:.2f}% \"\n",
    "              f\"(n={len(cers)})\")\n",
    "\n",
    "# Character-level analysis\n",
    "char_errors = {}\n",
    "for case in individual_errors:\n",
    "    for char in case['gt']:\n",
    "        if char not in char_errors:\n",
    "            char_errors[char] = []\n",
    "        char_errors[char].append(case['cer'])\n",
    "\n",
    "# Most problematic characters\n",
    "sorted_chars = sorted(char_errors.items(), key=lambda x: np.mean(x[1]), reverse=True)\n",
    "print(\"\\nTop 10 Most Problematic Characters:\")\n",
    "for char, cers in sorted_chars[:10]:\n",
    "    if len(cers) >= 5:  # Only if appears at least 5 times\n",
    "        print(f\"  '{char}': {np.mean(cers)*100:.2f}% (n={len(cers)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119afda8",
   "metadata": {},
   "source": [
    "## 18. Comprehensive Performance Report\n",
    "\n",
    "Generate final summary with all metrics, charts, and export results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
