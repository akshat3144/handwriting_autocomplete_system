HiGAN+ Handwriting Imitation
================================

This document summarizes the notebook implementation at `back_up_without_metrics.ipynb` and the supporting modules inside the `networks/`, `lib/`, and `metric/` packages. It is written so that each section can be lifted directly into presentation slides.


Project Snapshot
----------------
- **Goal**: Generate legible handwritten words that mimic a reference writer while matching requested text content.
- **Dataset**: IAM handwriting (HDF5 packed) accessed via `lib/datasets.py`.
- **Training Entry Point**: Notebook cell block 6 defines the main adversarial training loop.
- **Configuration**: `configs/gan_iam.yml` sets network hyperparameters and training knobs (batch size, loss weights, etc.).
- **Pretrained Auxiliaries**: OCR (`pretrained/ocr_iam_new.pth`) and Writer Identifier (`pretrained/wid_iam_new.pth`).


High-Level Pipeline
-------------------
1. **Style Extraction**: Encode a writer's style from a reference image (`StyleBackbone` + `StyleEncoder`).
2. **Content Preparation**: Convert target text to label sequences using `strLabelConverter` (`lib/alphabet.py`).
3. **Image Synthesis**: Generator fuses style vector and textual sequence to render handwriting conditioned on sequence length (`networks/model.py` dispatcher, `BigGAN_networks.py` layers).
4. **Discrimination**: Global (`Discriminator`) and patch-wise (`PatchDiscriminator`) critics judge realism.
5. **Auxiliary Supervision**: OCR (`Recognizer`) enforces readability; Writer Identifier enforces stylistic identity; contextual, reconstruction, and KL losses regularize synthesis.
6. **Metrics & Outputs**: History is logged, checkpoints saved in `checkpoints/`, visualizations written to PNGs.


Core Components
---------------

### Style Backbone (`networks/module.py`, class `StyleBackbone`)
- **Purpose**: Extract multi-scale convolutional features from grayscale handwriting strips, feeding both encoder and writer ID.
- **Structure**: Sequential Conv-BN-LeakyReLU blocks with stride-2 pooling to reduce width; residual links reuse features for stability.
- **Key Features**:
	- Channel progression (e.g., 64→128→256) captures increasing abstraction levels.
	- Adaptive average pooling normalizes variable-width inputs before fully connected heads.
	- Shared backbone ensures latent style and writer classifier perceive identical features.

### Style Encoder (`networks/module.py`, class `StyleEncoder`)
- **Purpose**: Map backbone feature pyramid to a compact latent style vector `z`.
- **Inner Flow**:
	- Receives backbone features plus sequence lengths (scaled by `cfg.char_width`).
	- Passes features through gated linear layers; optionally returns `(mu, logvar)` for VAE regularization.
	- Uses temporal pooling to handle variable-length sequences, outputting deterministic style embeddings when `vae_mode=False` or sampled latents when enabled.
- **Why It Matters**: Encapsulates writer-specific stroke patterns (thickness, slant, texture) in a vector that the generator can condition on, enabling consistent handwriting mimicry.

### Generator (`networks/BigGAN_networks.py`, class `Generator`)
- **Purpose**: Produce handwriting images conditioned on style vector and encoded text sequence.
- **Architecture Highlights**:
	- **Input Fusion**: Concatenates style embeddings with learned character embeddings generated by a positional encoder (`networks/utils.py` helper functions).
	- **Hierarchical Residual Blocks**: Uses BigGAN-style ResBlocks with spectral normalization removed (helps continuous tones). Blocks upsample width dimension progressively while injecting adaptive instance normalization (AdaIN) parameters derived from style vector.
	- **Self-Attention**: Optional attention layers (if enabled in config) focus on long-range stroke coherence for longer words.
	- **Output Layer**: Final conv projects to 1-channel image; `tanh` activation normalizes to [-1, 1].
- **Feature Reasoning**:
	- AdaIN ensures style-specific stroke characteristics modulate shared content features.
	- Skip connections stabilize GAN training and aid gradient flow across elongated word images.
	- Character embeddings align text positions with spatial locations, crucial for legibility.

### Discriminator (`networks/BigGAN_networks.py`, class `Discriminator`)
- **Purpose**: Judge global realism and text-style consistency.
- **Design**:
	- Downsampling ResBlocks mirror the generator but use spectral normalization to enforce Lipschitz constraints.
	- Accepts concatenated image and positional encodings; also receives label length to gate features.
	- Ends with linear projection to a scalar real/fake score.
- **Rationale**:
	- Multi-scale receptive fields verify stroke continuity across whole word.
	- Conditioning on label length prevents the generator from cheating with incorrect widths.

### Patch Discriminator (`networks/BigGAN_networks.py`, class `PatchDiscriminator`)
- **Purpose**: Focus on local texture realism and pen pressure variation.
- **Mechanism**:
	- Operates on fixed-size patches extracted via `networks/utils.extract_all_patches`.
	- Lightweight ConvNet that outputs patch-wise authenticity scores.
- **Reasoning**: Local critic counters blurriness and enforces high-frequency detail, complementing global discriminator.

### Recognizer (`networks/module.py`, class `Recognizer`)
- **Purpose**: OCR network ensuring generated text matches target labels via CTC loss.
- **Layout**:
	- Convolutional front-end creates feature maps; BiLSTM stack decodes temporal sequence; final linear layer projects to alphabet logits.
	- `len_scale` attribute records CTC down-sampling factor used when computing logit sequence length.
- **Why**: Without OCR supervision the generator might sacrifice readability; the recognizer anchors content correctness.

### Writer Identifier (`networks/module.py`, class `WriterIdentifier`)
- **Purpose**: Classify writer IDs, encouraging generated images to match reference writer traits.
- **Flow**:
	- Consumes shared backbone features.
	- Dense head with softmax outputs over writer classes; cross-entropy compares to ground-truth writer IDs.
- **Impact**: Prevents mode collapse on style; doubles as feature extractor for contextual loss.

### Loss Suite (`networks/loss.py` and notebook cell 6)
- **Adversarial Losses**: Hinge GAN losses for global and patch discriminators (`F.relu(1 ± score)`).
- **CTC Loss**: `torch.nn.CTCLoss` between recognizer logits and target labels (applied to random-style, style-guided, and reconstruction outputs).
- **Writer ID Loss**: Cross-entropy using `WriterIdentifier` over concatenated style and reconstruction images.
- **Reconstruction Loss**: `recn_l1_loss` compares reconstruction output and real input in pixel space.
- **Contextual Loss**: `CXLoss` matches high-level feature distributions between real and generated images.
- **KL Loss**: Optional VAE regularization (`KLloss`) encouraging latent space smoothness.
- **Hyperparameters**: Tuned via `cfg.training.lambda_ctx`, `lambda_kl`, and gradient penalty scalars inside the notebook loop.


Training Loop (Notebook)
------------------------
1. **Data Load**: `lib/datasets.get_dataset` returns style images, augmentations, labels, and writer IDs. Collate function handles variable-length padding.
2. **Critic Step** (`optimizer_D`):
	 - Freeze generator/encoder; synthesize fake, style-guided, and reconstruction images with `torch.no_grad()`.
	 - Compute global and patch discriminator losses on real (augmented) and fake batches.
3. **Generator Step** (`optimizer_G` every `num_critic_train` iters):
	 - Enable gradients on generator/encoder; resample text and style latents.
	 - Forward pass through recognizer, writer identifier, contextual loss pipeline.
	 - Aggregate losses with weights and backpropagate.
4. **Bookkeeping**: Track averaged losses per epoch, dump checkpoints to `checkpoints/epoch_X.pth` and `checkpoints/latest.pth`.
5. **Visualization & Metrics**: Subsequent cells export PNGs (`training_losses.png`, `inference_*.png`), compute OCR cer/wer, and save final model to `models/higanplus_trained.pth`.


Inference Modes (Notebook Section 9)
------------------------------------
- **Style-Guided**: Extract style from reference, regenerate same or custom text (Cell 9.1, saves `inference_style_guided.png`).
- **Random Style**: Sample normal-distributed style vectors, synthesize new handwriting (`inference_random_style.png`).
- **Style Interpolation**: Linearly interpolate between two style vectors to visualize stylistic morphing (`inference_interpolation.png`).
- **Same Style, Multiple Texts**: Repeat a single style vector to maintain handwriting identity across phrases (`inference_same_style.png`).
- **Custom Style Transfer**: Generate arbitrary phrases in the exact handwriting of a reference image (`inference_custom_style_transfer.png`).


Slide-Friendly Outline
----------------------
- **Slide 1**: Problem statement + dataset snapshot.
- **Slide 2**: End-to-end pipeline diagram (style encoder → generator → discriminators + aux losses).
- **Slide 3**: Style Backbone & Encoder architecture (layers, rationale for VAE option).
- **Slide 4**: Generator internals (ResBlocks, AdaIN, character embeddings).
- **Slide 5**: Discriminator duo (global vs patch) and hinge loss equations.
- **Slide 6**: Auxiliary networks (Recognizer, Writer Identifier) and loss terms.
- **Slide 7**: Training loop pseudocode and scheduler details.
- **Slide 8**: Results gallery (PNG outputs) + CER/WER metrics.
- **Slide 9**: Future improvements (e.g., diffusion refinements, curriculum learning, better contextual losses).


Setup & Repro Steps
-------------------
- Install dependencies: `pip install -r requirements.txt` (ensure CUDA-compatible torch).
- Place IAM HDF5 datasets under `data/iam/` as referenced in the notebook warning messages.
- Launch notebook, adjust `project_path` to match local directory, execute cells sequentially.
- Optional metrics scripts live inside `metric/`; they can be adapted for batch evaluation outside the notebook.


References
----------
- HiGAN paper: *Hierarchical Generative Adversarial Networks for Handwritten Text* (original inspiration).
- BigGAN architecture: Brock et al., *Large Scale GAN Training for High Fidelity Natural Image Synthesis*.
- CTC Loss: Graves et al., *Connectionist Temporal Classification*. 