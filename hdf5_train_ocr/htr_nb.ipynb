{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n",
      "Python: 3.13.5\n",
      "Platform: macOS-26.0.1-arm64-arm-64bit-Mach-O\n",
      "CPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "GPUs: []\n",
      "Using MPS: False\n",
      "Sanity matmul ran on /CPU:0 in 0.135s\n"
     ]
    }
   ],
   "source": [
    "# Apple Metal (MPS) setup for TensorFlow on macOS\n",
    "import os, platform, sys, time\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# List devices\n",
    "cpus = tf.config.list_physical_devices(\"CPU\")\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"CPUs:\", cpus)\n",
    "print(\"GPUs:\", gpus)\n",
    "\n",
    "# If a GPU is visible on macOS, it's the Metal (MPS) backend.\n",
    "USING_MPS = (len(gpus) > 0 and platform.system() == \"Darwin\")\n",
    "print(\"Using MPS:\", USING_MPS)\n",
    "\n",
    "# Optional: enable mixed precision if MPS is available (can speed up training)\n",
    "try:\n",
    "    if USING_MPS:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "        print(\"Mixed precision enabled (mixed_float16)\")\n",
    "except Exception as e:\n",
    "    print(\"Mixed precision not enabled:\", e)\n",
    "\n",
    "# Quick sanity compute on the fastest device available\n",
    "try:\n",
    "    device = \"/GPU:0\" if USING_MPS else \"/CPU:0\"\n",
    "    with tf.device(device):\n",
    "        a = tf.random.normal([4096, 4096])\n",
    "        b = tf.random.normal([4096, 4096])\n",
    "        t0 = time.time()\n",
    "        c = tf.matmul(a, b)\n",
    "        _ = c.numpy()\n",
    "        dt = time.time() - t0\n",
    "    print(f\"Sanity matmul ran on {device} in {dt:.3f}s\")\n",
    "except Exception as e:\n",
    "    print(\"Sanity compute failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-20T08:44:27.789856Z",
     "iopub.status.busy": "2025-10-20T08:44:27.789340Z",
     "iopub.status.idle": "2025-10-20T08:44:44.184208Z",
     "shell.execute_reply": "2025-10-20T08:44:44.183164Z",
     "shell.execute_reply.started": "2025-10-20T08:44:27.789828Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Handwritten Text Recognition Using CNN-LSTM\n",
      "================================================================================\n",
      "\n",
      "[1/7] Initializing character encoder...\n",
      "Vocabulary size: 70\n",
      "\n",
      "[2/7] Building CRNN model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CRNN_HTR\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CRNN_HTR\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">35,910</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_1 (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │     \u001b[38;5;34m1,049,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m70\u001b[0m)         │        \u001b[38;5;34m35,910\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,738,630</span> (33.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,738,630\u001b[0m (33.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,736,582</span> (33.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,736,582\u001b[0m (33.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> (8.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,048\u001b[0m (8.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters: 8,738,630\n",
      "\n",
      "[3/7] Model architecture successfully created!\n",
      "\n",
      "Note: To train the model, you need:\n",
      "  - IAM Dataset with image paths and labels\n",
      "  - Create train/validation data generators\n",
      "  - Call train_model() function\n",
      "\n",
      "[4/7] Example usage:\n",
      "\n",
      "    # Example: Load your dataset\n",
      "    train_images = ['path/to/image1.png', 'path/to/image2.png', ...]\n",
      "    train_labels = ['hello', 'world', ...]\n",
      "\n",
      "    # Create data generator\n",
      "    train_gen = HTRDataGenerator(train_images, train_labels, encoder, batch_size=5)\n",
      "    val_gen = HTRDataGenerator(val_images, val_labels, encoder, batch_size=5)\n",
      "\n",
      "    # Train model\n",
      "    history = train_model(model, train_gen, val_gen, epochs=50)\n",
      "\n",
      "    # Evaluate\n",
      "    predictions = model.predict(test_images)\n",
      "    decoded = decode_predictions(predictions, encoder)\n",
      "    \n",
      "\n",
      "[5/7] Preprocessing example:\n",
      "Preprocessed shape: (32, 128, 1)\n",
      "Value range: [0.00, 1.00]\n",
      "\n",
      "[6/7] Character encoding example:\n",
      "Original: Hello World\n",
      "Encoded: [34, 5, 12, 12, 15, 0, 49, 15, 18, 12, 4]\n",
      "Decoded: Hello World\n",
      "\n",
      "[7/7] Jaro-Winkler similarity example:\n",
      "'recognition' vs 'recogniton': 98.18%\n",
      "\n",
      "================================================================================\n",
      "Setup complete! Model ready for training.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Handwritten Text Recognition Using Deep Learning: A CNN-LSTM Approach\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def grayscale_conversion(image):\n",
    "    \"\"\"Convert RGB image to grayscale using weighted sum\"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        # Formula: I_gray = 0.2989*R + 0.5870*G + 0.1140*B\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    return gray\n",
    "\n",
    "\n",
    "def binarization(gray_image, method='otsu'):\n",
    "    \"\"\"Apply thresholding to convert grayscale to binary\"\"\"\n",
    "    if method == 'otsu':\n",
    "        _, binary = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    elif method == 'huang':\n",
    "        threshold = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_TRIANGLE)[0]\n",
    "        _, binary = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n",
    "    else:\n",
    "        _, binary = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    return binary\n",
    "\n",
    "\n",
    "def resize_and_pad(image, target_height=32, target_width=128):\n",
    "    \"\"\"Resize image to fixed dimensions with padding\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Calculate scaling factor to maintain aspect ratio\n",
    "    scale = min(target_height / h, target_width / w)\n",
    "    new_h, new_w = int(h * scale), int(w * scale)\n",
    "    \n",
    "    # Resize image\n",
    "    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Create padded image with white background\n",
    "    padded = np.ones((target_height, target_width), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Calculate padding offsets\n",
    "    y_offset = (target_height - new_h) // 2\n",
    "    x_offset = (target_width - new_w) // 2\n",
    "    \n",
    "    # Place resized image in center\n",
    "    padded[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized\n",
    "    \n",
    "    return padded\n",
    "\n",
    "\n",
    "def image_inversion(binary_image):\n",
    "    \"\"\"Invert pixel intensities (black text on white becomes white text on black)\"\"\"\n",
    "    inverted = 255 - binary_image\n",
    "    return inverted\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"Normalize pixel values to range [0, 1]\"\"\"\n",
    "    normalized = image.astype(np.float32) / 255.0\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, target_height=32, target_width=128):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    \n",
    "    # Step 1: Grayscale conversion\n",
    "    gray = grayscale_conversion(image)\n",
    "    \n",
    "    # Step 2: Binarization\n",
    "    binary = binarization(gray, method='otsu')\n",
    "    \n",
    "    # Step 3: Resize and pad\n",
    "    resized = resize_and_pad(binary, target_height, target_width)\n",
    "    \n",
    "    # Step 4: Image inversion (optional, depending on dataset)\n",
    "    # inverted = image_inversion(resized)\n",
    "    \n",
    "    # Step 5: Normalization\n",
    "    normalized = normalize_image(resized)\n",
    "    \n",
    "    # Add channel dimension\n",
    "    preprocessed = np.expand_dims(normalized, axis=-1)\n",
    "    \n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CHARACTER ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "class CharacterEncoder:\n",
    "    \"\"\"Encode and decode characters for model training\"\"\"\n",
    "    \n",
    "    def __init__(self, characters=None):\n",
    "        if characters is None:\n",
    "            # Default character set (lowercase + uppercase + digits + space)\n",
    "            self.characters = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?'-\"\n",
    "        else:\n",
    "            self.characters = characters\n",
    "        \n",
    "        # Create character to index mapping (start from 0)\n",
    "        # CTC blank token will be at index len(characters) automatically\n",
    "        self.char_to_num = {char: idx for idx, char in enumerate(self.characters)}\n",
    "        \n",
    "        # Create index to character mapping\n",
    "        self.num_to_char = {idx: char for char, idx in self.char_to_num.items()}\n",
    "        \n",
    "        # Vocab size includes all characters + blank token at the end\n",
    "        self.vocab_size = len(self.characters) + 1  # +1 for CTC blank\n",
    "        self.blank_token_idx = len(self.characters)  # Blank is last index\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to numerical indices (0 to len(characters)-1)\"\"\"\n",
    "        encoded = []\n",
    "        for char in text:\n",
    "            if char in self.char_to_num:\n",
    "                encoded.append(self.char_to_num[char])\n",
    "            # Skip unknown characters instead of using blank\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Decode numerical indices to text (skip blank token)\"\"\"\n",
    "        decoded = []\n",
    "        for idx in indices:\n",
    "            if idx < len(self.characters) and idx in self.num_to_char:\n",
    "                decoded.append(self.num_to_char[idx])\n",
    "            # Skip blank token (self.blank_token_idx) and unknown indices\n",
    "        return ''.join(decoded)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CNN-LSTM MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "def build_crnn_model(input_shape=(32, 128, 1), num_classes=79):\n",
    "    \"\"\"\n",
    "    Build CRNN model with 6 Conv layers and 2 BiLSTM layers\n",
    "    Architecture from the paper\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = layers.Input(shape=input_shape, name='input_1')\n",
    "    \n",
    "    # Convolutional Block 1\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2d')(input_layer)\n",
    "    x = layers.MaxPooling2D((2, 2), name='max_pooling2d')(x)\n",
    "    \n",
    "    # Convolutional Block 2\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2d_1')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='max_pooling2d_1')(x)\n",
    "    \n",
    "    # Convolutional Block 3\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv2d_2')(x)\n",
    "    \n",
    "    # Convolutional Block 4\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv2d_3')(x)\n",
    "    x = layers.MaxPooling2D((2, 1), name='max_pooling2d_2')(x)\n",
    "    \n",
    "    # Convolutional Block 5\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='conv2d_4')(x)\n",
    "    x = layers.BatchNormalization(name='batch_normalization')(x)\n",
    "    \n",
    "    # Convolutional Block 6\n",
    "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='conv2d_5')(x)\n",
    "    x = layers.BatchNormalization(name='batch_normalization_1')(x)\n",
    "    x = layers.MaxPooling2D((2, 1), name='max_pooling2d_3')(x)\n",
    "    \n",
    "    # Convolutional Block 7\n",
    "    x = layers.Conv2D(512, (2, 2), activation='relu', name='conv2d_6')(x)\n",
    "    \n",
    "    # Reshape for LSTM (Lambda layer)\n",
    "    # Shape: (batch, height, width, channels) -> (batch, width, height*channels)\n",
    "    x = layers.Lambda(lambda x: tf.squeeze(x, axis=1), name='lambda')(x)\n",
    "    \n",
    "    # Bidirectional LSTM layers\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.2), \n",
    "                            name='bidirectional')(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.2), \n",
    "                            name='bidirectional_1')(x)\n",
    "    \n",
    "    # Dense output layer\n",
    "    output = layers.Dense(num_classes, activation='softmax', name='dense')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=input_layer, outputs=output, name='CRNN_HTR')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CTC LOSS FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def ctc_loss_function(y_true, y_pred):\n",
    "    \"\"\"CTC (Connectionist Temporal Classification) loss\"\"\"\n",
    "    # Get batch size\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    \n",
    "    # Input length (time steps from model output)\n",
    "    input_length = tf.shape(y_pred)[1] * tf.ones(shape=(batch_size, 1), dtype='int32')\n",
    "    \n",
    "    # Label length (actual label sequence length)\n",
    "    label_length = tf.reduce_sum(tf.cast(y_true != 0, tf.int32), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Calculate CTC loss\n",
    "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. DATA GENERATOR\n",
    "# ============================================================================\n",
    "\n",
    "class HTRDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Data generator for HTR training\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, encoder, batch_size=5, \n",
    "                 img_height=32, img_width=128, shuffle=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.encoder = encoder\n",
    "        self.batch_size = batch_size\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.image_paths))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches per epoch\"\"\"\n",
    "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, indexes):\n",
    "        \"\"\"Generate data for a batch\"\"\"\n",
    "        # Initialize arrays\n",
    "        X = np.zeros((self.batch_size, self.img_height, self.img_width, 1), dtype=np.float32)\n",
    "        y = np.zeros((self.batch_size, 64), dtype=np.int32)  # Max label length = 64\n",
    "        \n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            # Load and preprocess image\n",
    "            img = preprocess_image(self.image_paths[idx], self.img_height, self.img_width)\n",
    "            X[i] = img\n",
    "            \n",
    "            # Encode label\n",
    "            encoded_label = self.encoder.encode(self.labels[idx])\n",
    "            y[i, :len(encoded_label)] = encoded_label\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. JARO-WINKLER SIMILARITY\n",
    "# ============================================================================\n",
    "\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    \"\"\"Calculate Jaro-Winkler similarity between two strings\"\"\"\n",
    "    \n",
    "    def jaro_similarity(s1, s2):\n",
    "        if len(s1) == 0 and len(s2) == 0:\n",
    "            return 1.0\n",
    "        if len(s1) == 0 or len(s2) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        match_distance = max(len(s1), len(s2)) // 2 - 1\n",
    "        s1_matches = [False] * len(s1)\n",
    "        s2_matches = [False] * len(s2)\n",
    "        \n",
    "        matches = 0\n",
    "        transpositions = 0\n",
    "        \n",
    "        for i in range(len(s1)):\n",
    "            start = max(0, i - match_distance)\n",
    "            end = min(i + match_distance + 1, len(s2))\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if s2_matches[j] or s1[i] != s2[j]:\n",
    "                    continue\n",
    "                s1_matches[i] = True\n",
    "                s2_matches[j] = True\n",
    "                matches += 1\n",
    "                break\n",
    "        \n",
    "        if matches == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        k = 0\n",
    "        for i in range(len(s1)):\n",
    "            if not s1_matches[i]:\n",
    "                continue\n",
    "            while not s2_matches[k]:\n",
    "                k += 1\n",
    "            if s1[i] != s2[k]:\n",
    "                transpositions += 1\n",
    "            k += 1\n",
    "        \n",
    "        return (matches / len(s1) + matches / len(s2) + \n",
    "                (matches - transpositions / 2) / matches) / 3\n",
    "    \n",
    "    jaro_sim = jaro_similarity(str1, str2)\n",
    "    \n",
    "    # Calculate prefix length (max 4)\n",
    "    prefix = 0\n",
    "    for i in range(min(len(str1), len(str2), 4)):\n",
    "        if str1[i] == str2[i]:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Calculate Jaro-Winkler similarity\n",
    "    jaro_winkler = jaro_sim + (prefix * 0.1 * (1 - jaro_sim))\n",
    "    \n",
    "    return jaro_winkler\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(model, train_generator, val_generator, epochs=50, \n",
    "                checkpoint_path='best_model.h5'):\n",
    "    \"\"\"Train the CRNN model\"\"\"\n",
    "    \n",
    "    # Compile model with SGD optimizer\n",
    "    optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=optimizer, loss=ctc_loss_function)\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. PREDICTION AND DECODING\n",
    "# ============================================================================\n",
    "\n",
    "def decode_predictions(predictions, encoder, blank_index=None):\n",
    "    \"\"\"Decode CTC predictions to text\n",
    "    \n",
    "    Args:\n",
    "        predictions: numpy array of shape (batch_size, time_steps, num_classes)\n",
    "        encoder: CharacterEncoder instance\n",
    "        blank_index: Index of blank token (optional, uses encoder.blank_token_idx)\n",
    "    \n",
    "    Returns:\n",
    "        List of decoded text strings\n",
    "    \"\"\"\n",
    "    if blank_index is None:\n",
    "        blank_index = encoder.blank_token_idx\n",
    "    \n",
    "    decoded_texts = []\n",
    "    \n",
    "    # predictions shape: (batch_size, time_steps, num_classes)\n",
    "    batch_size = predictions.shape[0]\n",
    "    time_steps = predictions.shape[1]\n",
    "    \n",
    "    # Create input_length for all samples in batch\n",
    "    input_lengths = np.full((batch_size,), time_steps, dtype=np.int32)\n",
    "    \n",
    "    # Decode all predictions at once\n",
    "    decoded, _ = tf.keras.backend.ctc_decode(\n",
    "        predictions,\n",
    "        input_length=input_lengths,\n",
    "        greedy=True\n",
    "    )\n",
    "    \n",
    "    # Convert to text\n",
    "    decoded = decoded[0].numpy()\n",
    "    for i in range(batch_size):\n",
    "        # Get the decoded sequence for this sample\n",
    "        seq = decoded[i]\n",
    "        # Decode to text using encoder\n",
    "        text = encoder.decode(seq)\n",
    "        decoded_texts.append(text)\n",
    "    \n",
    "    return decoded_texts\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 9. VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(history, epochs_trained):\n",
    "    \"\"\"Plot training and validation accuracy/loss\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history.history['loss'], label='Training Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'Training and Validation Loss ({epochs_trained} Epochs)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy (if available)\n",
    "    if 'accuracy' in history.history:\n",
    "        ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title(f'Training and Validation Accuracy ({epochs_trained} Epochs)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'training_history_{epochs_trained}_epochs.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_predictions(image_paths, true_labels, predicted_labels, num_samples=5):\n",
    "    \"\"\"Visualize predictions with original images\"\"\"\n",
    "    \n",
    "    num_samples = min(num_samples, len(image_paths))\n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=(12, num_samples * 2))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Load and display image\n",
    "        img = cv2.imread(image_paths[i], cv2.IMREAD_GRAYSCALE)\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Calculate similarity\n",
    "        similarity = jaro_winkler_similarity(true_labels[i], predicted_labels[i])\n",
    "        \n",
    "        # Set title with true and predicted text\n",
    "        title = f\"True: {true_labels[i]}\\nPredicted: {predicted_labels[i]}\\nSimilarity: {similarity:.2%}\"\n",
    "        axes[i].set_title(title, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_samples.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 10. MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Handwritten Text Recognition Using CNN-LSTM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize character encoder\n",
    "    print(\"\\n[1/7] Initializing character encoder...\")\n",
    "    encoder = CharacterEncoder()\n",
    "    print(f\"Vocabulary size: {encoder.vocab_size}\")\n",
    "    \n",
    "    # Build model\n",
    "    print(\"\\n[2/7] Building CRNN model...\")\n",
    "    model = build_crnn_model(input_shape=(32, 128, 1), num_classes=encoder.vocab_size)\n",
    "    model.summary()\n",
    "    \n",
    "    # Print model statistics\n",
    "    total_params = model.count_params()\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    \n",
    "    print(\"\\n[3/7] Model architecture successfully created!\")\n",
    "    print(\"\\nNote: To train the model, you need:\")\n",
    "    print(\"  - IAM Dataset with image paths and labels\")\n",
    "    print(\"  - Create train/validation data generators\")\n",
    "    print(\"  - Call train_model() function\")\n",
    "    \n",
    "    print(\"\\n[4/7] Example usage:\")\n",
    "    print(\"\"\"\n",
    "    # Example: Load your dataset\n",
    "    train_images = ['path/to/image1.png', 'path/to/image2.png', ...]\n",
    "    train_labels = ['hello', 'world', ...]\n",
    "    \n",
    "    # Create data generator\n",
    "    train_gen = HTRDataGenerator(train_images, train_labels, encoder, batch_size=5)\n",
    "    val_gen = HTRDataGenerator(val_images, val_labels, encoder, batch_size=5)\n",
    "    \n",
    "    # Train model\n",
    "    history = train_model(model, train_gen, val_gen, epochs=50)\n",
    "    \n",
    "    # Evaluate\n",
    "    predictions = model.predict(test_images)\n",
    "    decoded = decode_predictions(predictions, encoder)\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n[5/7] Preprocessing example:\")\n",
    "    # Create a sample image for demonstration\n",
    "    sample_img = np.ones((50, 200, 3), dtype=np.uint8) * 255\n",
    "    cv2.putText(sample_img, \"SAMPLE\", (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "    cv2.imwrite('sample_input.png', sample_img)\n",
    "    \n",
    "    # Preprocess it\n",
    "    preprocessed = preprocess_image('sample_input.png')\n",
    "    print(f\"Preprocessed shape: {preprocessed.shape}\")\n",
    "    print(f\"Value range: [{preprocessed.min():.2f}, {preprocessed.max():.2f}]\")\n",
    "    \n",
    "    print(\"\\n[6/7] Character encoding example:\")\n",
    "    sample_text = \"Hello World\"\n",
    "    encoded = encoder.encode(sample_text)\n",
    "    decoded = encoder.decode(encoded)\n",
    "    print(f\"Original: {sample_text}\")\n",
    "    print(f\"Encoded: {encoded}\")\n",
    "    print(f\"Decoded: {decoded}\")\n",
    "    \n",
    "    print(\"\\n[7/7] Jaro-Winkler similarity example:\")\n",
    "    text1 = \"recognition\"\n",
    "    text2 = \"recogniton\"\n",
    "    similarity = jaro_winkler_similarity(text1, text2)\n",
    "    print(f\"'{text1}' vs '{text2}': {similarity:.2%}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Setup complete! Model ready for training.\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return model, encoder\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, encoder = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T08:44:44.187080Z",
     "iopub.status.busy": "2025-10-20T08:44:44.186396Z",
     "iopub.status.idle": "2025-10-20T09:43:19.815042Z",
     "shell.execute_reply": "2025-10-20T09:43:19.814133Z",
     "shell.execute_reply.started": "2025-10-20T08:44:44.187054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HANDWRITTEN TEXT RECOGNITION - IAM DATASET TRAINING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 1: LOADING DATASET\n",
      "================================================================================\n",
      "Loading training/validation from: /Users/raghav_sarna/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/handwriting_autocomplete_system/hdf5_train_ocr/data/iam/trnvalset_words64_OrgSz.hdf5\n",
      "Loading test set from: /Users/raghav_sarna/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/handwriting_autocomplete_system/hdf5_train_ocr/data/iam/testset_words64_OrgSz.hdf5\n",
      "\n",
      "================================================================================\n",
      "DATASET STATISTICS (HDF5)\n",
      "================================================================================\n",
      "Train/Val pool samples: 52,231\n",
      "Test samples: 11,170\n",
      "Unique characters: 69\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 2: PREPARING DATA\n",
      "================================================================================\n",
      "\n",
      "Splitting data: Train/Val/Test\n",
      "✓ Train: 37736 samples (72.2%)\n",
      "✓ Validation: 6660 samples (12.8%)\n",
      "✓ Test: 7835 samples (15.0%)\n",
      "\n",
      "✓ Character set created: 70 unique characters\n",
      "Characters:  \"'+,-./0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz...\n",
      "\n",
      "✓ Encoder vocabulary size: 71\n",
      "✓ Encoder saved to: encoder_20251106_185537.pkl\n",
      "\n",
      "================================================================================\n",
      "STEP 3: CREATING DATA GENERATORS\n",
      "================================================================================\n",
      "✓ Training batches: 1179\n",
      "✓ Validation batches: 208\n",
      "\n",
      "================================================================================\n",
      "STEP 4: BUILDING CTC MODEL\n",
      "================================================================================\n",
      "\n",
      "CTC Model Architecture:\n",
      "Total inputs: 4\n",
      "  - input_data: (None, 32, 128, 1)\n",
      "  - y_true: (None, 64)\n",
      "  - input_length: (None, 1)\n",
      "  - label_length: (None, 1)\n",
      "\n",
      "Base model summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CRNN_HTR\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CRNN_HTR\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,423</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_1 (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │     \u001b[38;5;34m1,049,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m71\u001b[0m)         │        \u001b[38;5;34m36,423\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,739,143</span> (33.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,739,143\u001b[0m (33.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,737,095</span> (33.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,737,095\u001b[0m (33.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> (8.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,048\u001b[0m (8.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: TRAINING MODEL\n",
      "================================================================================\n",
      "Epochs: 50\n",
      "Batch size: 32\n",
      "Model will be saved to: htr_model_20251106_185537.weights.h5\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghav_sarna/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 15.7560\n",
      "Epoch 1: val_loss improved from None to 10.70400, saving model to htr_model_20251106_185537.weights.weights.h5\n",
      "\u001b[1m1179/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 299ms/step - loss: 13.6118 - val_loss: 10.7040 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1042/1179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m39s\u001b[0m 291ms/step - loss: 9.6263"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 811\u001b[39m\n\u001b[32m    807\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 744\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    741\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel will be saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_SAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    742\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m history = \u001b[43mtrain_ctc_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_SAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(HISTORY_SAVE_PATH, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    747\u001b[39m     pickle.dump(history.history, f)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 390\u001b[39m, in \u001b[36mtrain_ctc_model\u001b[39m\u001b[34m(model, train_generator, val_generator, epochs, checkpoint_path)\u001b[39m\n\u001b[32m    388\u001b[39m early_stopping = EarlyStopping(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m10\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[32m1\u001b[39m)\n\u001b[32m    389\u001b[39m reduce_lr = ReduceLROnPlateau(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.5\u001b[39m, patience=\u001b[32m5\u001b[39m, min_lr=\u001b[32m1e-6\u001b[39m, verbose=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/.venv/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training Script for Handwritten Text Recognition using IAM Dataset (HDF5 or Kaggle files)\n",
    "This script can load the local HDF5 IAM dataset under data/iam or the Kaggle-style folder.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "\n",
    "# NOTE: The model, preprocessing, and visualization helpers are defined in Cell 1\n",
    "# We will reuse: CharacterEncoder, build_crnn_model, decode_predictions,\n",
    "# plot_training_history, visualize_predictions, jaro_winkler_similarity, preprocess_image\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IAM DATASET CONFIGURATION (Kaggle-style folders) - kept for optional use\n",
    "# ============================================================================\n",
    "\n",
    "class IAMDatasetConfig:\n",
    "    \"\"\"Configuration for IAM Dataset paths (Kaggle folder layout)\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.words_dir = self.base_path / 'iam_words' / 'words'\n",
    "        self.lines_dir = self.base_path / 'iam_words' / 'lines'  # May not exist\n",
    "        self.words_txt = self.words_dir / 'words.txt'\n",
    "        if not self.words_txt.exists():\n",
    "            self.words_txt = self.base_path / 'words_new.txt'\n",
    "        if not self.words_txt.exists():\n",
    "            self.words_txt = self.base_path / 'iam_words' / 'words_new.txt'\n",
    "        self.lines_txt = self.words_dir / 'lines.txt'\n",
    "        if not self.lines_txt.exists():\n",
    "            self.lines_txt = self.base_path / 'lines_new.txt'\n",
    "    \n",
    "    def validate(self):\n",
    "        print(f\"\\nValidating dataset at: {self.base_path}\")\n",
    "        if not self.base_path.exists():\n",
    "            raise FileNotFoundError(f\"Base path does not exist: {self.base_path}\")\n",
    "        if not self.words_dir.exists():\n",
    "            raise FileNotFoundError(f\"Words directory not found: {self.words_dir}\")\n",
    "        if not self.words_txt.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Annotation file not found. Checked:\\n\"\n",
    "                f\"  - {self.base_path / 'words.txt'}\\n\"\n",
    "                f\"  - {self.base_path / 'words_new.txt'}\\n\"\n",
    "                f\"  - {self.base_path / 'ascii' / 'words.txt'}\\n\"\n",
    "                f\"Please ensure words.txt exists in the dataset root.\"\n",
    "            )\n",
    "        print(f\"✓ Base path: {self.base_path}\")\n",
    "        print(f\"✓ Words directory: {self.words_dir}\")\n",
    "        print(f\"✓ Annotation file: {self.words_txt}\")\n",
    "        print(\"✓ IAM Dataset paths validated successfully\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. IAM DATASET LOADER (Kaggle) - kept for optional use\n",
    "# ============================================================================\n",
    "\n",
    "class IAMDatasetLoader:\n",
    "    \"\"\"Load and parse IAM Handwriting Dataset (Kaggle files)\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.data = []\n",
    "    \n",
    "    def parse_words_file(self, max_samples=None):\n",
    "        print(f\"\\nLoading IAM words from: {self.config.words_txt}\")\n",
    "        if not self.config.words_txt.exists():\n",
    "            raise FileNotFoundError(f\"Words file not found: {self.config.words_txt}\")\n",
    "        data = []\n",
    "        skipped = 0\n",
    "        with open(self.config.words_txt, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#') or len(line.strip()) == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 9:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    word_id = parts[0]\n",
    "                    status = parts[1]\n",
    "                    transcription = ' '.join(parts[8:])\n",
    "                    if status != 'ok' or len(transcription.strip()) == 0:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    parts_id = word_id.split('-')\n",
    "                    folder1 = parts_id[0]\n",
    "                    folder2 = '-'.join(parts_id[:2])\n",
    "                    image_path = self.config.words_dir / folder1 / folder2 / f\"{word_id}.png\"\n",
    "                    if not image_path.exists():\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    data.append({\n",
    "                        'word_id': word_id,\n",
    "                        'image_path': str(image_path),\n",
    "                        'transcription': transcription,\n",
    "                        'status': status\n",
    "                    })\n",
    "                    if max_samples and len(data) >= max_samples:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "        print(f\"✓ Loaded {len(data)} samples, skipped {skipped} samples\")\n",
    "        self.data = data\n",
    "        return data\n",
    "    \n",
    "    def parse_lines_file(self, max_samples=None):\n",
    "        print(f\"\\nLoading IAM lines from: {self.config.lines_txt}\")\n",
    "        if not self.config.lines_txt.exists():\n",
    "            raise FileNotFoundError(f\"Lines file not found: {self.config.lines_txt}\")\n",
    "        data = []\n",
    "        skipped = 0\n",
    "        with open(self.config.lines_txt, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#') or len(line.strip()) == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 9:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    line_id = parts[0]\n",
    "                    status = parts[1]\n",
    "                    transcription = ' '.join(parts[8:])\n",
    "                    if status != 'ok' or len(transcription.strip()) == 0:\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    parts_id = line_id.split('-')\n",
    "                    folder1 = parts_id[0]\n",
    "                    folder2 = f\"{parts_id[0]}-{parts_id[1]}\"\n",
    "                    image_path = self.config.lines_dir / folder1 / folder2 / f\"{line_id}.png\"\n",
    "                    if not image_path.exists():\n",
    "                        skipped += 1\n",
    "                        continue\n",
    "                    data.append({\n",
    "                        'line_id': line_id,\n",
    "                        'image_path': str(image_path),\n",
    "                        'transcription': transcription,\n",
    "                        'status': status\n",
    "                    })\n",
    "                    if max_samples and len(data) >= max_samples:\n",
    "                        break\n",
    "                except Exception:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "        print(f\"✓ Loaded {len(data)} samples, skipped {skipped} samples\")\n",
    "        self.data = data\n",
    "        return data\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        if not self.data:\n",
    "            print(\"No data loaded yet!\")\n",
    "            return\n",
    "        transcriptions = [item['transcription'] for item in self.data]\n",
    "        lengths = [len(t) for t in transcriptions]\n",
    "        all_chars = set(''.join(transcriptions))\n",
    "        stats = {\n",
    "            'total_samples': len(self.data),\n",
    "            'min_length': min(lengths),\n",
    "            'max_length': max(lengths),\n",
    "            'avg_length': np.mean(lengths),\n",
    "            'unique_chars': len(all_chars),\n",
    "            'characters': sorted(all_chars)\n",
    "        }\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total samples: {stats['total_samples']:,}\")\n",
    "        print(f\"Text length - Min: {stats['min_length']}, Max: {stats['max_length']}, Avg: {stats['avg_length']:.2f}\")\n",
    "        print(f\"Unique characters: {stats['unique_chars']}\")\n",
    "        print(f\"Character set: {''.join(stats['characters'][:50])}...\")\n",
    "        print(\"=\"*80)\n",
    "        return stats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DATA PREPARATION (common)\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_data_for_training(data, test_size=0.15, val_size=0.15, random_state=42):\n",
    "    print(f\"\\nSplitting data: Train/Val/Test\")\n",
    "    train_val_data, test_data = train_test_split(\n",
    "        data, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_val_data, test_size=val_size, random_state=random_state\n",
    "    )\n",
    "    print(f\"✓ Train: {len(train_data)} samples ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "    print(f\"✓ Validation: {len(val_data)} samples ({len(val_data)/len(data)*100:.1f}%)\")\n",
    "    print(f\"✓ Test: {len(test_data)} samples ({len(test_data)/len(data)*100:.1f}%)\")\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def create_character_set(data):\n",
    "    all_text = ' '.join([item['transcription'] for item in data])\n",
    "    unique_chars = sorted(set(all_text))\n",
    "    char_string = ''.join(unique_chars)\n",
    "    print(f\"\\n✓ Character set created: {len(unique_chars)} unique characters\")\n",
    "    print(f\"Characters: {char_string[:100]}...\")\n",
    "    return char_string\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CTC-COMPATIBLE GENERATORS (paths and arrays)\n",
    "# ============================================================================\n",
    "\n",
    "class CTCDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"CTC-compatible data generator for images referenced by file paths\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, encoder, batch_size=32, \n",
    "                 img_height=32, img_width=128, shuffle=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.encoder = encoder\n",
    "        self.batch_size = batch_size\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.image_paths))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X_dict, y = self.__data_generation(indexes)\n",
    "        return X_dict, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, indexes):\n",
    "        valid_samples = []\n",
    "        for idx in indexes:\n",
    "            try:\n",
    "                img = preprocess_image(self.image_paths[idx], self.img_height, self.img_width)\n",
    "                encoded_label = self.encoder.encode(self.labels[idx])\n",
    "                label_len = len(encoded_label)\n",
    "                if label_len == 0:\n",
    "                    continue\n",
    "                valid_samples.append((img, encoded_label, label_len))\n",
    "            except Exception as e:\n",
    "                print(f\"\\nWarning: Skipping image {self.image_paths[idx]}: {e}\")\n",
    "                continue\n",
    "        if len(valid_samples) == 0:\n",
    "            valid_samples = [(np.zeros((self.img_height, self.img_width, 1), dtype=np.float32), [0], 1)]\n",
    "        actual_batch_size = min(len(valid_samples), self.batch_size)\n",
    "        X = np.zeros((actual_batch_size, self.img_height, self.img_width, 1), dtype=np.float32)\n",
    "        y_labels = np.ones((actual_batch_size, 64), dtype=np.int32) * -1\n",
    "        input_length = np.ones((actual_batch_size, 1), dtype=np.int32) * 31\n",
    "        label_length = np.zeros((actual_batch_size, 1), dtype=np.int32)\n",
    "        for i in range(actual_batch_size):\n",
    "            img, encoded_label, label_len = valid_samples[i]\n",
    "            X[i] = img\n",
    "            y_labels[i, :label_len] = encoded_label\n",
    "            label_length[i] = label_len\n",
    "        return {\n",
    "            'input_data': X,\n",
    "            'y_true': y_labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length\n",
    "        }, np.zeros([actual_batch_size])\n",
    "\n",
    "\n",
    "def preprocess_image_array(img_array, target_height=32, target_width=128):\n",
    "    \"\"\"Preprocess a grayscale image array to model input shape.\"\"\"\n",
    "    # Ensure 2D grayscale\n",
    "    if img_array.ndim == 3:\n",
    "        img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n",
    "    gray = img_array\n",
    "    binary = binarization(gray, method='otsu')\n",
    "    resized = resize_and_pad(binary, target_height, target_width)\n",
    "    normalized = normalize_image(resized)\n",
    "    return np.expand_dims(normalized, axis=-1)\n",
    "\n",
    "\n",
    "class CTCArrayDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"CTC-compatible data generator for images provided as numpy arrays\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, encoder, batch_size=32, \n",
    "                 img_height=32, img_width=128, shuffle=True):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.encoder = encoder\n",
    "        self.batch_size = batch_size\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.images))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.images) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X_dict, y = self.__data_generation(indexes)\n",
    "        return X_dict, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, indexes):\n",
    "        valid_samples = []\n",
    "        for idx in indexes:\n",
    "            try:\n",
    "                img = preprocess_image_array(self.images[idx], self.img_height, self.img_width)\n",
    "                encoded_label = self.encoder.encode(self.labels[idx])\n",
    "                label_len = len(encoded_label)\n",
    "                if label_len == 0:\n",
    "                    continue\n",
    "                valid_samples.append((img, encoded_label, label_len))\n",
    "            except Exception as e:\n",
    "                print(f\"\\nWarning: Skipping array image idx={idx}: {e}\")\n",
    "                continue\n",
    "        if len(valid_samples) == 0:\n",
    "            valid_samples = [(np.zeros((self.img_height, self.img_width, 1), dtype=np.float32), [0], 1)]\n",
    "        actual_batch_size = min(len(valid_samples), self.batch_size)\n",
    "        X = np.zeros((actual_batch_size, self.img_height, self.img_width, 1), dtype=np.float32)\n",
    "        y_labels = np.ones((actual_batch_size, 64), dtype=np.int32) * -1\n",
    "        input_length = np.ones((actual_batch_size, 1), dtype=np.int32) * 31\n",
    "        label_length = np.zeros((actual_batch_size, 1), dtype=np.int32)\n",
    "        for i in range(actual_batch_size):\n",
    "            img, encoded_label, label_len = valid_samples[i]\n",
    "            X[i] = img\n",
    "            y_labels[i, :label_len] = encoded_label\n",
    "            label_length[i] = label_len\n",
    "        return {\n",
    "            'input_data': X,\n",
    "            'y_true': y_labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length\n",
    "        }, np.zeros([actual_batch_size])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CTC MODEL WRAPPER\n",
    "# ============================================================================\n",
    "\n",
    "def build_ctc_model(input_shape=(32, 128, 1), num_classes=78):\n",
    "    \"\"\"Build CTC model with proper loss handling\"\"\"\n",
    "    from tensorflow.keras import layers, Model\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    input_data = layers.Input(name='input_data', shape=input_shape)\n",
    "    y_true = layers.Input(name='y_true', shape=[64], dtype='int32')\n",
    "    input_length = layers.Input(name='input_length', shape=[1], dtype='int32')\n",
    "    label_length = layers.Input(name='label_length', shape=[1], dtype='int32')\n",
    "    base_model = build_crnn_model(input_shape, num_classes)\n",
    "    y_pred = base_model(input_data)\n",
    "    ctc_loss = layers.Lambda(\n",
    "        lambda args: tf.keras.backend.ctc_batch_cost(args[0], args[1], args[2], args[3]),\n",
    "        output_shape=(1,),\n",
    "        name='ctc'\n",
    "    )([y_true, y_pred, input_length, label_length])\n",
    "    model = Model(inputs=[input_data, y_true, input_length, label_length], outputs=ctc_loss)\n",
    "    model.base_model = base_model\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_ctc_model(model, train_generator, val_generator, epochs=50, checkpoint_path='best_model.h5'):\n",
    "    \"\"\"Train the CTC model\"\"\"\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss={'ctc': lambda y_true, y_pred: y_pred})\n",
    "    weights_path = checkpoint_path.replace('.h5', '.weights.h5') if checkpoint_path.endswith('.h5') else checkpoint_path + '.weights.h5'\n",
    "    checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1, save_weights_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "    history = model.fit(train_generator, validation_data=val_generator, epochs=epochs, callbacks=[checkpoint, early_stopping, reduce_lr], verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. HDF5 DATASET LOADER (local data/iam/*.hdf5)\n",
    "# ============================================================================\n",
    "\n",
    "def load_hdf5_samples(h5_path, max_samples=None):\n",
    "    \"\"\"Load samples from an HDF5 file with IAM format used in this repo.\n",
    "    Expected keys: imgs, lbs, img_seek_idxs, lb_seek_idxs, img_lens, lb_lens.\n",
    "    Returns a list of dicts with 'image' (H x W uint8) and 'transcription' (str).\n",
    "    \"\"\"\n",
    "    h5_path = Path(h5_path)\n",
    "    if not h5_path.exists():\n",
    "        raise FileNotFoundError(f\"HDF5 file not found: {h5_path}\")\n",
    "    samples = []\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        keys = set(f.keys())\n",
    "        # Case 1: concatenated storage\n",
    "        if {'imgs','lbs','img_seek_idxs','lb_seek_idxs','img_lens','lb_lens'}.issubset(keys):\n",
    "            imgs = f['imgs'][:]\n",
    "            lbs = f['lbs'][:]\n",
    "            img_seek = f['img_seek_idxs'][:]\n",
    "            img_lens = f['img_lens'][:]\n",
    "            lb_seek = f['lb_seek_idxs'][:]\n",
    "            lb_lens = f['lb_lens'][:]\n",
    "            n = len(img_lens)\n",
    "            for i in range(n):\n",
    "                x0 = img_seek[i]\n",
    "                w = int(img_lens[i])\n",
    "                img = imgs[:, x0:x0+w]\n",
    "                y0 = lb_seek[i]\n",
    "                l = int(lb_lens[i])\n",
    "                text = ''.join(chr(int(c)) for c in lbs[y0:y0+l])\n",
    "                samples.append({'image': img, 'transcription': text})\n",
    "                if max_samples and len(samples) >= max_samples:\n",
    "                    break\n",
    "        # Case 2: naive arrays per-sample\n",
    "        elif 'images' in keys and 'labels' in keys:\n",
    "            images = f['images'][:]\n",
    "            labels = f['labels'][:]\n",
    "            for i in range(len(images)):\n",
    "                img = images[i]\n",
    "                lab = labels[i]\n",
    "                if isinstance(lab, bytes):\n",
    "                    text = lab.decode('utf-8')\n",
    "                else:\n",
    "                    text = str(lab)\n",
    "                samples.append({'image': img, 'transcription': text})\n",
    "                if max_samples and len(samples) >= max_samples:\n",
    "                    break\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized HDF5 structure in {h5_path}. Keys: {sorted(keys)}\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. EVALUATION FUNCTIONS (support paths or arrays)\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_model(model, test_data, encoder, batch_size=32, max_samples=100):\n",
    "    \"\"\"Evaluate model on test set. Supports test_data items with 'image_path' or 'image'.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATING MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    if max_samples:\n",
    "        test_data = test_data[:max_samples]\n",
    "    image_paths = []\n",
    "    images = []\n",
    "    true_labels = [item['transcription'] for item in test_data]\n",
    "    print(f\"Preprocessing {len(true_labels)} samples...\")\n",
    "    for i, item in enumerate(test_data):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(true_labels)}...\")\n",
    "        if 'image_path' in item:\n",
    "            image_paths.append(item['image_path'])\n",
    "            img = preprocess_image(item['image_path'])\n",
    "        else:\n",
    "            image_paths.append(None)\n",
    "            img = preprocess_image_array(item['image'])\n",
    "        images.append(img)\n",
    "    images = np.array(images)\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    prediction_model = model.base_model if hasattr(model, 'base_model') else model\n",
    "    predictions = prediction_model.predict(images, batch_size=batch_size, verbose=1)\n",
    "    print(\"Decoding predictions...\")\n",
    "    predicted_labels = decode_predictions(predictions, encoder)\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    similarities = []\n",
    "    correct_predictions = 0\n",
    "    cer_scores = []\n",
    "    wer_scores = []\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        sim = jaro_winkler_similarity(true, pred)\n",
    "        similarities.append(sim)\n",
    "        if true.strip() == pred.strip():\n",
    "            correct_predictions += 1\n",
    "        cer_scores.append(calculate_cer(true, pred))\n",
    "        wer_scores.append(calculate_wer(true, pred))\n",
    "    results = {\n",
    "        'num_samples': len(test_data),\n",
    "        'exact_match_accuracy': correct_predictions / len(test_data) * 100,\n",
    "        'avg_similarity': np.mean(similarities) * 100,\n",
    "        'median_similarity': np.median(similarities) * 100,\n",
    "        'min_similarity': np.min(similarities) * 100,\n",
    "        'avg_cer': np.mean(cer_scores) * 100,\n",
    "        'avg_wer': np.mean(wer_scores) * 100,\n",
    "        'predictions': list(zip(image_paths, true_labels, predicted_labels, similarities)),\n",
    "        'images': [item.get('image', None) for item in test_data]\n",
    "    }\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Samples evaluated: {results['num_samples']}\")\n",
    "    print(f\"Exact match accuracy: {results['exact_match_accuracy']:.2f}%\")\n",
    "    print(f\"Average similarity: {results['avg_similarity']:.2f}%\")\n",
    "    print(f\"Median similarity: {results['median_similarity']:.2f}%\")\n",
    "    print(f\"Character Error Rate (CER): {results['avg_cer']:.2f}%\")\n",
    "    print(f\"Word Error Rate (WER): {results['avg_wer']:.2f}%\")\n",
    "    print(\"=\"*80)\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_cer(true_text, pred_text):\n",
    "    distance = levenshtein_distance(true_text, pred_text)\n",
    "    cer = distance / max(len(true_text), 1)\n",
    "    return cer\n",
    "\n",
    "\n",
    "def calculate_wer(true_text, pred_text):\n",
    "    true_words = true_text.split()\n",
    "    pred_words = pred_text.split()\n",
    "    distance = levenshtein_distance(true_words, pred_words)\n",
    "    wer = distance / max(len(true_words), 1)\n",
    "    return wer\n",
    "\n",
    "\n",
    "def levenshtein_distance(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix[x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix[0, y] = y\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix[x, y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix[x, y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1] + 1,\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "    return int(matrix[size_x - 1, size_y - 1])\n",
    "\n",
    "\n",
    "def save_predictions_to_file(results, output_path='predictions.txt'):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"PREDICTION RESULTS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        for img_path, true, pred, sim in results['predictions']:\n",
    "            f.write(f\"Image: {img_path}\\n\")\n",
    "            f.write(f\"True: {true}\\n\")\n",
    "            f.write(f\"Pred: {pred}\\n\")\n",
    "            f.write(f\"Similarity: {sim*100:.2f}%\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\\n\")\n",
    "    print(f\"✓ Predictions saved to: {output_path}\")\n",
    "\n",
    "\n",
    "def visualize_predictions_arrays(images, true_labels, predicted_labels, num_samples=5):\n",
    "    \"\"\"Visualize predictions when we have images as arrays instead of file paths.\"\"\"\n",
    "    num_samples = min(num_samples, len(images))\n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=(12, num_samples * 2))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    for i in range(num_samples):\n",
    "        img = images[i]\n",
    "        if img.ndim == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        similarity = jaro_winkler_similarity(true_labels[i], predicted_labels[i])\n",
    "        title = f\"True: {true_labels[i]}\\nPredicted: {predicted_labels[i]}\\nSimilarity: {similarity:.2%}\"\n",
    "        axes[i].set_title(title, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_samples.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 9. MAIN TRAINING SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline (defaults to local HDF5 in data/iam)\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"HANDWRITTEN TEXT RECOGNITION - IAM DATASET TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # CONFIGURATION\n",
    "    # ----------------------------------------\n",
    "    BASE_DIR = Path.cwd()  # Notebook lives in hdf5_train_ocr/\n",
    "    HDF5_DIR = BASE_DIR / 'data' / 'iam'\n",
    "    TRNVAL_H5 = HDF5_DIR / 'trnvalset_words64_OrgSz.hdf5'\n",
    "    TEST_H5 = HDF5_DIR / 'testset_words64_OrgSz.hdf5'\n",
    "\n",
    "    # Set to False to fall back to Kaggle-style folder loading (if you have it)\n",
    "    USE_HDF5 = True\n",
    "\n",
    "    # Training parameters\n",
    "    USE_LINES = False  # retained for Kaggle path (ignored for HDF5 words)\n",
    "    MAX_SAMPLES = None  # e.g., 5000 for quick test; None to use all\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "    IMG_HEIGHT = 32\n",
    "    IMG_WIDTH = 128\n",
    "\n",
    "    # Output paths\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    MODEL_SAVE_PATH = f'htr_model_{timestamp}.weights.h5'\n",
    "    ENCODER_SAVE_PATH = f'encoder_{timestamp}.pkl'\n",
    "    HISTORY_SAVE_PATH = f'history_{timestamp}.pkl'\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # STEP 1: LOAD DATASET\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 1: LOADING DATASET\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    if USE_HDF5:\n",
    "        # Validate paths\n",
    "        if not TRNVAL_H5.exists() or not TEST_H5.exists():\n",
    "            print(f\"\\n❌ HDF5 files not found. Expected:\\n  - {TRNVAL_H5}\\n  - {TEST_H5}\")\n",
    "            print(\"Please ensure the IAM HDF5 files are present under data/iam.\")\n",
    "            return\n",
    "        # Load training/validation pool and test set\n",
    "        print(f\"Loading training/validation from: {TRNVAL_H5}\")\n",
    "        trnval_samples = load_hdf5_samples(TRNVAL_H5, max_samples=MAX_SAMPLES)\n",
    "        print(f\"Loading test set from: {TEST_H5}\")\n",
    "        test_data = load_hdf5_samples(TEST_H5)\n",
    "        # Gather quick stats for HDF5\n",
    "        all_texts = [s['transcription'] for s in trnval_samples]\n",
    "        unique_chars = sorted(set(''.join(all_texts)))\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DATASET STATISTICS (HDF5)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Train/Val pool samples: {len(trnval_samples):,}\")\n",
    "        print(f\"Test samples: {len(test_data):,}\")\n",
    "        print(f\"Unique characters: {len(unique_chars)}\")\n",
    "        print(\"=\"*80)\n",
    "        data = trnval_samples\n",
    "    else:\n",
    "        # Kaggle-style fallback (kept intact)\n",
    "        IAM_DATASET_PATH = r\"/kaggle/input/iam-handwriting-word-database\"\n",
    "        try:\n",
    "            config = IAMDatasetConfig(IAM_DATASET_PATH)\n",
    "            config.validate()\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"\\n❌ ERROR: {e}\")\n",
    "            print(\"\\n⚠️  Please update IAM_DATASET_PATH in the script to your dataset location!\")\n",
    "            return\n",
    "        loader = IAMDatasetLoader(config)\n",
    "        data = loader.parse_lines_file(max_samples=MAX_SAMPLES) if USE_LINES else loader.parse_words_file(max_samples=MAX_SAMPLES)\n",
    "        if len(data) == 0:\n",
    "            print(\"❌ No data loaded! Please check your dataset paths.\")\n",
    "            return\n",
    "        _ = loader.get_statistics()\n",
    "        test_data = data  # for Kaggle we will resplit below\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # STEP 2: PREPARE DATA\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 2: PREPARING DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    train_data, val_data, test_data_split = prepare_data_for_training(data) if not USE_HDF5 else prepare_data_for_training(trnval_samples)\n",
    "    if USE_HDF5:\n",
    "        # For HDF5 we already have a dedicated test set; use it\n",
    "        train_data, val_data, _ = train_data, val_data, test_data_split\n",
    "    else:\n",
    "        test_data = test_data_split\n",
    "\n",
    "    # Create character encoder from the combined pool (train+val for HDF5, all for Kaggle)\n",
    "    char_set = create_character_set(train_data + val_data)\n",
    "    encoder = CharacterEncoder(characters=char_set)\n",
    "    print(f\"\\n✓ Encoder vocabulary size: {encoder.vocab_size}\")\n",
    "    with open(ENCODER_SAVE_PATH, 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    print(f\"✓ Encoder saved to: {ENCODER_SAVE_PATH}\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # STEP 3: CREATE DATA GENERATORS\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 3: CREATING DATA GENERATORS\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    if USE_HDF5:\n",
    "        train_images = [item['image'] for item in train_data]\n",
    "        train_labels = [item['transcription'] for item in train_data]\n",
    "        val_images = [item['image'] for item in val_data]\n",
    "        val_labels = [item['transcription'] for item in val_data]\n",
    "        train_generator = CTCArrayDataGenerator(train_images, train_labels, encoder, batch_size=BATCH_SIZE, img_height=IMG_HEIGHT, img_width=IMG_WIDTH, shuffle=True)\n",
    "        val_generator = CTCArrayDataGenerator(val_images, val_labels, encoder, batch_size=BATCH_SIZE, img_height=IMG_HEIGHT, img_width=IMG_WIDTH, shuffle=False)\n",
    "    else:\n",
    "        train_images = [item['image_path'] for item in train_data]\n",
    "        train_labels = [item['transcription'] for item in train_data]\n",
    "        val_images = [item['image_path'] for item in val_data]\n",
    "        val_labels = [item['transcription'] for item in val_data]\n",
    "        train_generator = CTCDataGenerator(train_images, train_labels, encoder, batch_size=BATCH_SIZE, img_height=IMG_HEIGHT, img_width=IMG_WIDTH, shuffle=True)\n",
    "        val_generator = CTCDataGenerator(val_images, val_labels, encoder, batch_size=BATCH_SIZE, img_height=IMG_HEIGHT, img_width=IMG_WIDTH, shuffle=False)\n",
    "\n",
    "    print(f\"✓ Training batches: {len(train_generator)}\")\n",
    "    print(f\"✓ Validation batches: {len(val_generator)}\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # STEP 4: BUILD MODEL\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 4: BUILDING CTC MODEL\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    model = build_ctc_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 1), num_classes=encoder.vocab_size)\n",
    "    print(\"\\nCTC Model Architecture:\")\n",
    "    print(f\"Total inputs: {len(model.inputs)}\")\n",
    "    print(f\"  - input_data: {model.inputs[0].shape}\")\n",
    "    print(f\"  - y_true: {model.inputs[1].shape}\")\n",
    "    print(f\"  - input_length: {model.inputs[2].shape}\")\n",
    "    print(f\"  - label_length: {model.inputs[3].shape}\")\n",
    "    print(f\"\\nBase model summary:\")\n",
    "    model.base_model.summary()\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # STEP 5: TRAIN MODEL\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 5: TRAINING MODEL\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Epochs: {EPOCHS}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Model will be saved to: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    history = train_ctc_model(model, train_generator, val_generator, epochs=EPOCHS, checkpoint_path=MODEL_SAVE_PATH)\n",
    "\n",
    "    with open(HISTORY_SAVE_PATH, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(f\"\\n✓ Training history saved to: {HISTORY_SAVE_PATH}\")\n",
    "\n",
    "    base_model_path = MODEL_SAVE_PATH.replace('.weights.h5', '_base.h5')\n",
    "    model.base_model.save(base_model_path)\n",
    "    print(f\"✓ Base model saved to: {base_model_path}\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # STEP 6: PLOT TRAINING HISTORY\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 6: PLOTTING TRAINING HISTORY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    plot_training_history(history, EPOCHS)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # STEP 7: EVALUATE MODEL\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 7: EVALUATING MODEL\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    results = evaluate_model(model, test_data if USE_HDF5 else test_data, encoder, batch_size=BATCH_SIZE, max_samples=100)\n",
    "\n",
    "    predictions_file = f'predictions_{timestamp}.txt'\n",
    "    save_predictions_to_file(results, predictions_file)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # STEP 8: VISUALIZE SAMPLE PREDICTIONS\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 8: VISUALIZING PREDICTIONS\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    sample_count = min(5, results['num_samples'])\n",
    "    sample_indices = np.random.choice(results['num_samples'], sample_count, replace=False)\n",
    "    sample_true = [results['predictions'][i][1] for i in sample_indices]\n",
    "    sample_pred = [results['predictions'][i][2] for i in sample_indices]\n",
    "\n",
    "    if USE_HDF5:\n",
    "        sample_imgs = [results['images'][i] for i in sample_indices]\n",
    "        visualize_predictions_arrays(sample_imgs, sample_true, sample_pred, num_samples=sample_count)\n",
    "    else:\n",
    "        sample_img_paths = [results['predictions'][i][0] for i in sample_indices]\n",
    "        visualize_predictions(sample_img_paths, sample_true, sample_pred, num_samples=sample_count)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # FINAL SUMMARY\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"✓ Model saved: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"✓ Encoder saved: {ENCODER_SAVE_PATH}\")\n",
    "    print(f\"✓ History saved: {HISTORY_SAVE_PATH}\")\n",
    "    print(f\"✓ Predictions saved: {predictions_file}\")\n",
    "    print(f\"\\nFinal Performance:\")\n",
    "    print(f\"  - Character Error Rate (CER): {results['avg_cer']:.2f}%\")\n",
    "    print(f\"  - Word Error Rate (WER): {results['avg_wer']:.2f}%\")\n",
    "    print(f\"  - Average Similarity: {results['avg_similarity']:.2f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5_DIR: /Users/raghav_sarna/Desktop/Drive/Plaksha/Semester 5/DL/handwriting/handwriting_autocomplete_system/hdf5_train_ocr/data/iam\n",
      "trnval exists? True\n",
      "test exists? True\n",
      "Loaded samples: 3\n",
      "0: image shape=(64, 132), text=\"Mr.\"\n",
      "1: image shape=(64, 284), text=\"MOVE\"\n",
      "2: image shape=(64, 136), text=\"stop\"\n",
      "Loaded samples: 3\n",
      "0: image shape=(64, 132), text=\"Mr.\"\n",
      "1: image shape=(64, 284), text=\"MOVE\"\n",
      "2: image shape=(64, 136), text=\"stop\"\n"
     ]
    }
   ],
   "source": [
    "# Quick smoke test: verify HDF5 files exist and load a few samples\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "HDF5_DIR = BASE_DIR / 'data' / 'iam'\n",
    "TRNVAL_H5 = HDF5_DIR / 'trnvalset_words64_OrgSz.hdf5'\n",
    "TEST_H5 = HDF5_DIR / 'testset_words64_OrgSz.hdf5'\n",
    "\n",
    "print('HDF5_DIR:', HDF5_DIR)\n",
    "print('trnval exists?', TRNVAL_H5.exists())\n",
    "print('test exists?', TEST_H5.exists())\n",
    "\n",
    "# Define a minimal fallback loader here so this cell works even if previous cell hasn't run yet.\n",
    "try:\n",
    "    _ = load_hdf5_samples  # noqa: F401\n",
    "except NameError:\n",
    "    import h5py\n",
    "    def load_hdf5_samples(h5_path, max_samples=None):\n",
    "        h5_path = Path(h5_path)\n",
    "        if not h5_path.exists():\n",
    "            raise FileNotFoundError(f\"HDF5 file not found: {h5_path}\")\n",
    "        samples = []\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            keys = set(f.keys())\n",
    "            if {'imgs','lbs','img_seek_idxs','lb_seek_idxs','img_lens','lb_lens'}.issubset(keys):\n",
    "                imgs = f['imgs'][:]\n",
    "                lbs = f['lbs'][:]\n",
    "                img_seek = f['img_seek_idxs'][:]\n",
    "                img_lens = f['img_lens'][:]\n",
    "                lb_seek = f['lb_seek_idxs'][:]\n",
    "                lb_lens = f['lb_lens'][:]\n",
    "                n = len(img_lens)\n",
    "                for i in range(n):\n",
    "                    x0 = int(img_seek[i])\n",
    "                    w = int(img_lens[i])\n",
    "                    img = imgs[:, x0:x0+w]\n",
    "                    y0 = int(lb_seek[i])\n",
    "                    l = int(lb_lens[i])\n",
    "                    text = ''.join(chr(int(c)) for c in lbs[y0:y0+l])\n",
    "                    samples.append({'image': img, 'transcription': text})\n",
    "                    if max_samples and len(samples) >= max_samples:\n",
    "                        break\n",
    "            elif 'images' in keys and 'labels' in keys:\n",
    "                images = f['images'][:]\n",
    "                labels = f['labels'][:]\n",
    "                for i in range(len(images)):\n",
    "                    img = images[i]\n",
    "                    lab = labels[i]\n",
    "                    if isinstance(lab, bytes):\n",
    "                        text = lab.decode('utf-8')\n",
    "                    else:\n",
    "                        text = str(lab)\n",
    "                    samples.append({'image': img, 'transcription': text})\n",
    "                    if max_samples and len(samples) >= max_samples:\n",
    "                        break\n",
    "            else:\n",
    "                raise ValueError(f\"Unrecognized HDF5 structure in {h5_path}. Keys: {sorted(keys)}\")\n",
    "        return samples\n",
    "\n",
    "if TRNVAL_H5.exists():\n",
    "    samples = load_hdf5_samples(TRNVAL_H5, max_samples=3)\n",
    "    print('Loaded samples:', len(samples))\n",
    "    for i, s in enumerate(samples):\n",
    "        img = s['image']\n",
    "        txt = s['transcription']\n",
    "        print(f'{i}: image shape={img.shape}, text=\"{txt[:30]}\"')\n",
    "else:\n",
    "    print('HDF5 files not found under data/iam.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (407040242.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimport tensorflow as tf\\nprint('GPUs:', tf.config.list_physical_devices('GPU'))\\nprint('Hello from quick output test')\u001b[39m\n                            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('GPUs:', tf.config.list_physical_devices('GPU'))\n",
    "print('Hello from quick output test')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1347338,
     "sourceId": 2243895,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
